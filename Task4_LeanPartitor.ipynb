{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from local_packages import *\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type ='deepseek'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['qwen']:\n",
    "        version = 'long'\n",
    "        service=QwenService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '32k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['deepseek']:\n",
    "        version = 'coder'\n",
    "        service = DeepSeekService(version)\n",
    "    elif service_type in ['huida']:\n",
    "        version = 'gpt-4o'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = HuidaService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)\n",
    "\n",
    "js=JSProcessor()\n",
    "\n",
    "class ParseError(Exception):\n",
    "    def __init__(self, code, message=\"解析失败\"):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割Lean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pygments import lex\n",
    "from pygments.lexers import LeanLexer\n",
    "from pygments.token import Token\n",
    "\n",
    "# 指定目录路径\n",
    "directory_path = 'D:\\\\Joining\\\\Mathlib'\n",
    "\n",
    "# 遍历目录及其子目录中的所有 .txt 文件\n",
    "txt_files = []\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            txt_files.append(os.path.join(root, file))\n",
    "\n",
    "# 初始化 JSON 列表\n",
    "json_dict_list = []\n",
    "\n",
    "# 遍历所有找到的 .txt 文件并进行处理\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 使用 LeanLexer 解析代码\n",
    "    lexer = LeanLexer()\n",
    "    tokens = list(lex(content, lexer))\n",
    "    \n",
    "    # 初始化变量\n",
    "    current_segment = []\n",
    "    segments = []\n",
    "    start_new_segment = False\n",
    "    theorem_line = \"\"\n",
    "    para_index = 0\n",
    "    \n",
    "    # 遍历词法单元\n",
    "    for token_type, token_value in tokens:\n",
    "        if token_type == Token.Keyword.Declaration and token_value == 'theorem':\n",
    "            print(1)\n",
    "            if current_segment:\n",
    "                segments.append(''.join(current_segment))\n",
    "                para_index += 1\n",
    "                json_dict_list.append({\n",
    "                    \"txt_file_name\": os.path.relpath(txt_file, directory_path),\n",
    "                    \"para_content\": ''.join(current_segment),\n",
    "                    \"para_index\": para_index\n",
    "                })\n",
    "            current_segment = []\n",
    "            start_new_segment = True\n",
    "            theorem_line = token_value\n",
    "        \n",
    "        if start_new_segment:\n",
    "            current_segment.append(token_value)\n",
    "            start_new_segment = False\n",
    "        elif current_segment:\n",
    "            current_segment.append(token_value)\n",
    "    \n",
    "    # 添加最后一个段落\n",
    "    if current_segment:\n",
    "        segments.append(''.join(current_segment))\n",
    "        para_index += 1\n",
    "        para_content = ''.join(current_segment)\n",
    "        theorem_line = para_content.split('\\n')[0].strip()\n",
    "        json_dict_list.append({\n",
    "            \"txt_file_name\": os.path.basename(txt_file),\n",
    "            \"para_content\": para_content,\n",
    "            \"para_index\": para_index\n",
    "        })\n",
    "\n",
    "# 将结果写入 JSON 文件\n",
    "with open('output.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(json_dict_list, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 (Jiaoy)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
