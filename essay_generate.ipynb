{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from LLM_API import GLMService, SenseService, KimiService\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type = 'zhipu'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '8k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 论文写作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "#Analysis of format\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "def split_points(text):\n",
    "    # 使用正则表达式按照序号加句点的格式进行分割\n",
    "    points = re.findall(r'\\d+\\.\\s+(.+?)(?=\\n|\\Z)', text, re.DOTALL)\n",
    "    \n",
    "    return points\n",
    "\n",
    "def parse_outline(outline_str):\n",
    "    try:\n",
    "        outline_dict = json.loads(outline_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"无法解析提供的一二级目录框架，请确保其是有效的 JSON 格式。\")\n",
    "        return None\n",
    "    \n",
    "    # 检查解析后的字典是否符合预期的结构\n",
    "    if not isinstance(outline_dict, dict):\n",
    "        print(\"一二级目录框架不是有效的字典格式。\")\n",
    "        return None\n",
    "\n",
    "    # 可以进一步处理解析后的 outline_dict，根据需要进行操作，比如验证键值对是否符合预期等\n",
    "    # 以下是示例代码，你可以根据具体需求进行修改\n",
    "    chapters = []\n",
    "    for chapter, sub_chapters in outline_dict.items():\n",
    "        if sub_chapters is None:  # 如果子章节为 None，表示该大章节下没有小章节\n",
    "            sub_chapters = ['']   # 添加一个空白的小标题\n",
    "        elif not isinstance(sub_chapters, list):\n",
    "            print(f\"章节 '{chapter}' 的子章节不是有效的列表格式。\")\n",
    "            continue\n",
    "        \n",
    "        # 追加二级章节的关键点作为三级目录\n",
    "        sub_chapters_with_keypoints = []\n",
    "        for sub_chapter in sub_chapters:\n",
    "            if isinstance(sub_chapter, dict) and \"title\" in sub_chapter and \"key_points\" in sub_chapter:\n",
    "                sub_chapters_with_keypoints.append((sub_chapter[\"title\"], sub_chapter[\"key_points\"]))\n",
    "            else:\n",
    "                sub_chapters_with_keypoints.append((sub_chapter, []))  # 若未提供关键点，则为空列表\n",
    "        \n",
    "        chapters.append((chapter, sub_chapters_with_keypoints))\n",
    "    \n",
    "    return chapters\n",
    "\n",
    "def split_sentences(chapter_content):\n",
    "    # 初始化结果列表\n",
    "    sentences = []\n",
    "\n",
    "    # 使用换行符来分割文段\n",
    "    lines = chapter_content.split('\\n')\n",
    "\n",
    "    # 遍历每一行文本\n",
    "    for line in lines:\n",
    "        # 如果当前行不为空，则添加到结果列表中\n",
    "        if line.strip():\n",
    "            sentences.append(line.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "#Process_rounds\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "def get_first_level_outline(theme, service):\n",
    "    \"\"\"\n",
    "    获取论文的一级目录信息。\n",
    "\n",
    "    参数：\n",
    "    - theme：论文主题，字符串类型。\n",
    "    - service：用于与用户交互的服务对象。\n",
    "\n",
    "    返回值：\n",
    "    - 一级目录信息，字典类型。\n",
    "    \"\"\"\n",
    "    input_text = f'''\n",
    "    主题：“{theme}”。请根据该主题，以规范格式设计一篇论文的一二级目录框架。\n",
    "    请按以下格式提供一二级目录框架，并确保每个二级章节都包含一个标题：\n",
    "    {{\n",
    "        \"一级章节1\": \n",
    "            {{\n",
    "                 \"二级章节1\":[],\n",
    "                 \"二级章节2\":[],\n",
    "                 ...\n",
    "            }},\n",
    "            ...\n",
    "        ,\n",
    "        \"一级章节2\":             \n",
    "            {{\n",
    "                 \"二级章节1\":[],\n",
    "                 \"二级章节2\":[]\n",
    "            }},\n",
    "        ...\n",
    "    }}\n",
    "    '''\n",
    "\n",
    "\n",
    "    while True:\n",
    "        first_level_outline = service.ask_once(input_text)\n",
    "        # 尝试直接解析JSON\n",
    "        try:\n",
    "            first_level_outline_json = json.loads(first_level_outline)\n",
    "            return first_level_outline_json\n",
    "        except json.JSONDecodeError:\n",
    "            # 尝试从包裹符号中提取JSON内容再解析\n",
    "            json_content_match = re.search(r'```json\\s*(.*?)\\s*```', first_level_outline, re.DOTALL)\n",
    "            if json_content_match:\n",
    "                json_content = json_content_match.group(1)\n",
    "                try:\n",
    "                    first_level_outline_json = json.loads(json_content)\n",
    "                    return first_level_outline_json\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"返回的一级目录框架不是有效的 JSON 格式，请重新生成:\",first_level_outline)\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"返回的一级目录框架不是有效的 JSON 格式，请重新生成。\",first_level_outline)\n",
    "                continue\n",
    "\n",
    "def get_second_level_outline(theme, first_level_outline, service):\n",
    "    \"\"\"\n",
    "    获取论文的二级目录信息。\n",
    "\n",
    "    参数：\n",
    "    - theme：论文主题，字符串类型。\n",
    "    - first_level_outline：一级目录信息，字典类型。\n",
    "    - service：用于与用户交互的服务对象。\n",
    "\n",
    "    返回值：\n",
    "    - 论文的二级目录信息，字典类型。\n",
    "    \"\"\"\n",
    "    second_level_outline = {}\n",
    "    for first_level, second_level_list in first_level_outline.items():\n",
    "        parsed_second_level_list = []\n",
    "        for second_level_title in second_level_list:\n",
    "            while True:\n",
    "                input_prompt = f'''\n",
    "                主题：{theme}。对于当前章节\"{first_level}\"，请提供关于\"{second_level_title}\"这个二级章节的关键点，每个点是一个词：\n",
    "\n",
    "                请确保你提供的关键点符合以下要求，并以JSON格式返回：\n",
    "                {{\n",
    "                    \"title\": \"{second_level_title}\",\n",
    "                    \"key_points\": [\"关键点1\", \"关键点2\", ...]\n",
    "                }}\n",
    "                '''\n",
    "                key_points = service.ask_once(input_prompt)\n",
    "                try:\n",
    "                    key_points_json = json.loads(key_points)\n",
    "                    # 只在成功解析后添加到列表，避免重复添加\n",
    "                    parsed_second_level_list.append({second_level_title : key_points_json[\"key_points\"]})\n",
    "                    break\n",
    "                except json.JSONDecodeError:\n",
    "                    json_content_match = re.search(r'```json\\s*(.*?)\\s*```', key_points, re.DOTALL)\n",
    "                    if json_content_match:\n",
    "                        json_content = json_content_match.group(1)\n",
    "                        try:\n",
    "                            key_points_json = json.loads(json_content)\n",
    "                            parsed_second_level_list.append({second_level_title : key_points_json[\"key_points\"]})\n",
    "                            break\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(\"输入的关键点不是有效的 JSON 格式，请重新输入。\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(\"无法从输入中提取有效的 JSON 内容，请重新输入。\")\n",
    "                        continue\n",
    "        second_level_outline[first_level] = parsed_second_level_list\n",
    "    return second_level_outline\n",
    "\n",
    "def extract_previous_content(first_level_name, second_level_name, second_level_outline):\n",
    "    previous_content_list = []\n",
    "\n",
    "    # 标志位，表示是否找到了指定的一级章节和二级章节\n",
    "    found = False\n",
    "\n",
    "    for first_level, second_level_dict in second_level_outline.items():\n",
    "        if found:\n",
    "            break\n",
    "\n",
    "        for second_level, content in second_level_dict.items():\n",
    "            # 如果找到了指定的一级章节和二级章节\n",
    "            if first_level == first_level_name and second_level == second_level_name:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "            # 将当前一级章节和二级章节前面的所有信息组合成字符串并添加到列表中\n",
    "            previous_content_list.append(f\"{first_level}{second_level}{str(content)}\")\n",
    "\n",
    "    return previous_content_list\n",
    "\n",
    "def extract_key_points_from_outline(outline):\n",
    "    \"\"\"\n",
    "    从二级目录中提取所有三级关键词。\n",
    "    \n",
    "    :param outline: 二级目录的结构，一个包含引言和其他部分的字典。\n",
    "    :return: 一个包含所有三级关键词的列表。\n",
    "    \"\"\"\n",
    "    key_points_list = []\n",
    "    # 遍历outline中的所有部分\n",
    "    for section in outline.get('引言', []):\n",
    "        # 尝试解析key_points字段中的JSON字符串\n",
    "        try:\n",
    "            key_points_data = json.loads(section.get('key_points', '{}'))\n",
    "            # 如果存在key_points字段，则提取关键词\n",
    "            key_points = key_points_data.get('key_points', [])\n",
    "            key_points_list.extend(key_points)\n",
    "        except json.JSONDecodeError:\n",
    "            # 如果解析失败，跳过该部分\n",
    "            continue\n",
    "    return key_points_list\n",
    "\n",
    "def query_information_based_on_key_points(outline):\n",
    "    \"\"\"\n",
    "    基于二级目录中的关键词进行信息查询。\n",
    "    \n",
    "    :param outline: 二级目录的结构，一个包含引言和其他部分的字典。\n",
    "    \"\"\"\n",
    "    # 首先，从outline中提取所有三级关键词\n",
    "    key_points = extract_key_points_from_outline(outline)\n",
    "    \n",
    "    # 示例：打印所有关键词，实际操作中应替换为查询操作\n",
    "    for key_point in key_points:\n",
    "        print(f\"进行查询：{key_point}\")\n",
    "        # 假设的查询函数，您需要替换为实际的查询操作\n",
    "        # results = perform_query(key_point)\n",
    "        # 处理查询结果\n",
    "        # process_results(results)\n",
    "\n",
    "def generate_sub_chapter_points(theme, chapter_title, sub_chapters_list, window_length, service):\n",
    "    \"\"\"\n",
    "    生成小章节要点。\n",
    "\n",
    "    参数：\n",
    "    - theme：论文主题，字符串类型。\n",
    "    - chapter_title：大章节标题，字符串类型。\n",
    "    - sub_chapters_list：小章节标题列表，列表类型。\n",
    "    - window_length：对话总结长度，整数类型。\n",
    "    - service：生成内容所用的服务对象。\n",
    "\n",
    "    返回值：\n",
    "    - sub_chapter_points：小章节要点列表，列表类型。\n",
    "    \"\"\"\n",
    "    sub_chapter_points = []\n",
    "    for sub_chapter in sub_chapters_list:\n",
    "        # 生成小章节要点的请求文本\n",
    "        input_text = f'主题：“{theme}”，当前大章节：“{chapter_title}”，当前小章节：“{sub_chapter}”。请你作为本专业的专家，根据章节标题，提供论述此小章节的几个要点。'\n",
    "        # 生成小章节要点\n",
    "        points = service.ask_once(input_text)\n",
    "        sub_chapter_points.append(points)\n",
    "    return sub_chapter_points\n",
    "\n",
    "def generate_chapter_content(theme, chapter_title, sub_chapter_title,history, keywords, window_length, service):\n",
    "    \"\"\"\n",
    "    生成章节内容。\n",
    "\n",
    "    参数：\n",
    "    - theme：论文主题，字符串类型。\n",
    "    - outline：全文结构，字典类型。\n",
    "    - chapter_title：当前章节标题，字符串类型。\n",
    "    - sub_chapter_title：当前小章节标题，字符串类型。\n",
    "    - window_length：窗口长度，整数类型。\n",
    "    - service：用于与用户交互的服务对象。\n",
    "\n",
    "    返回值：\n",
    "    - 章节内容，列表类型。\n",
    "    \"\"\"\n",
    "    bool, sections_content_summary = summarize_conversation(window_length, history, service)\n",
    "    print('前文信息为：',sections_content_summary)\n",
    "    # 构建输入文本\n",
    "    input_text = f'主题：“{theme}”，当前章节：“{chapter_title}”，当前小章节：“{sub_chapter_title}”。请你作为本专业的专家，根据小章节标题，以学术论文的风格，围绕以下关键点{str(keywords)},详细阐述此小章节的内容。请确保讨论专注且深入，言之有物。知道上文已经论述了{sections_content_summary}，请你继续扩展论述，保持内容的连贯性和一致性。。'\n",
    "    \n",
    "    # 与用户交互，获取章节内容\n",
    "    chapter_content_text = service.ask_once(input_text)\n",
    "    print('生成章节为：',chapter_content_text)\n",
    "\n",
    "    return sections_content_summary, chapter_content_text\n",
    "\n",
    "def generate_markdown_essay(essay_dict):\n",
    "    markdown_essay = \"\"\n",
    "\n",
    "    for first_level, second_level_dict in essay_dict:\n",
    "        # 一级标题\n",
    "        markdown_essay += f\"# {first_level}\\n\\n\"\n",
    "        \n",
    "        for second_level, content in second_level_dict:\n",
    "            # 二级标题\n",
    "            markdown_essay += f\"## {second_level}\\n\\n\"\n",
    "            # 正文内容\n",
    "            markdown_essay += content + \"\\n\\n\"\n",
    "    \n",
    "    return markdown_essay\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "#Main_func\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "def generate_essay(theme, window_length, service):\n",
    "    \"\"\"\n",
    "    生成论文的主函数，接受主题、对话长度、服务对象作为参数，并返回生成的论文内容和预估成本。\n",
    "\n",
    "    参数：\n",
    "    - theme：论文主题，字符串类型。\n",
    "    - window_length：对话总结长度，整数类型。\n",
    "    - service：生成内容所用的服务对象。\n",
    "\n",
    "    返回值：\n",
    "    - essay_full：生成的论文内容，字符串类型。\n",
    "    - estimated_cost：生成论文的预估成本，浮点数类型。\n",
    "    \"\"\"\n",
    "    # 记录初始使用的token数\n",
    "    initial_tokens = service.total_tokens_used\n",
    "    # 初始化论文内容字典\n",
    "\n",
    "    # 获取论文一级目录\n",
    "    first_level_outline = get_first_level_outline(theme, service)\n",
    "    # 获取论文二级目录\n",
    "    second_level_outline = get_second_level_outline(theme, first_level_outline, service)\n",
    "\n",
    "    previous_content_list=[]\n",
    "    essay_dict = {}\n",
    "\n",
    "    for current_first_level, second_level_list in second_level_outline.items():\n",
    "        # 遍历每个一级标题及其对应的二级标题和关键词\n",
    "        current_level_essay = {}\n",
    "        for section in second_level_list:\n",
    "            for current_second_level, keywords in section.items():\n",
    "                # 更新当前的一级标题和二级标题\n",
    "                previous_content_list.append(f\"{current_first_level}：{current_second_level}：关键词为{str(keywords)}\")\n",
    "                current_summary, current_essay_content = generate_chapter_content(theme, current_first_level, current_second_level, previous_content_list, keywords, window_length, service)\n",
    "                previous_content_list=[current_summary]    \n",
    "                current_level_essay[current_second_level] = current_essay_content\n",
    "        \n",
    "        # 存储当前一级标题下所有二级标题的内容到 essay_dict 中\n",
    "        essay_dict[current_first_level] = current_level_essay\n",
    "\n",
    "    # 计算预估成本\n",
    "    final_tokens = service.total_tokens_used\n",
    "    tokens_used = final_tokens - initial_tokens\n",
    "    estimated_cost = tokens_used*0.000005 \n",
    "    print(essay_dict)\n",
    "    return essay_dict, estimated_cost\n",
    "\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "#Summarizer\n",
    "\n",
    "#————————————————————————————————————————————————————\n",
    "\n",
    "def summarize_conversation(window_length, conversation_list, service):\n",
    "    \"\"\"\n",
    "    对当前的对话背景信息进行总结，以减少长度，并保证essential_conversations的内容不被修改。\n",
    "    \"\"\"\n",
    "        # 检查对话列表是否为空\n",
    "    if not conversation_list:\n",
    "        return False, ''\n",
    "    essential_conversations = [conversation_list[-1]]\n",
    "    essential_length = sum(len(conv) for conv in essential_conversations)\n",
    "\n",
    "    if essential_length > window_length:\n",
    "        print(\"[系统]: 输入过长，请减少必要对话内容的长度。\")\n",
    "        return False, ''\n",
    "\n",
    "    non_essential_conversations = conversation_list[0:-1]\n",
    "    if not non_essential_conversations:\n",
    "        final_conversation=''\n",
    "        for conversation in essential_conversations:\n",
    "            final_conversation += conversation\n",
    "        return True, final_conversation\n",
    "\n",
    "    summarized_content = non_essential_conversations\n",
    "    while essential_length + sum(len(conv) for conv in summarized_content) > window_length:\n",
    "        storage = []\n",
    "        current_summary = \"\"\n",
    "        for conv in summarized_content:\n",
    "            if len(current_summary) + len(conv) > window_length:\n",
    "                summary = service.ask_once(f\"请总结，保留要点即可，越精炼越好：{current_summary}\") if current_summary else \"\"\n",
    "                storage.append(summary)\n",
    "                current_summary = conv\n",
    "            else:\n",
    "                current_summary += \" \" + conv\n",
    "\n",
    "        if current_summary:\n",
    "            summary = service.ask_once(f\"请总结，保留要点即可，越精炼越好：{current_summary}\")\n",
    "            storage.append(summary)\n",
    "        summarized_content = storage\n",
    "        \n",
    "    new_total_length = essential_length + sum(len(conv) for conv in summarized_content)\n",
    "    if new_total_length <= window_length:\n",
    "        # 将 summarized_content 列表转换为字符串\n",
    "        summarized_content_str = ' '.join(summarized_content)\n",
    "        # 构造最终的字符串，这里假设 conversation_list 的首尾元素已经是字符串\n",
    "        summarized_conversations = conversation_list[0] + summarized_content_str + conversation_list[-1]\n",
    "        return True, summarized_conversations\n",
    "    else:\n",
    "        # 如果处理后长度仍超过限制，可能需要进一步的处理逻辑或错误处理\n",
    "        return False, \"无法在给定窗口长度内完成总结\"\n",
    "\n",
    "theme = '论述美国对发展数字教育政策和措施'\n",
    "essay_dict, estimate_cost=generate_essay(theme, 1200, service)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析环节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in essay_dict:\n",
    "    for j in essay_dict[i]:\n",
    "        print(j)\n",
    "def generate_markdown_essay(essay_dict):\n",
    "    markdown_essay = \"\"\n",
    "\n",
    "    for first_level in essay_dict:\n",
    "        # 一级标题\n",
    "        markdown_essay += f\"# {first_level}\\n\\n\"\n",
    "        \n",
    "        for second_level in essay_dict[first_level]:\n",
    "            content=essay_dict[first_level][second_level]\n",
    "            # 二级标题\n",
    "            markdown_essay += f\"## {second_level}\\n\\n\"\n",
    "            # 正文内容\n",
    "            markdown_essay += content + \"\\n\\n\"\n",
    "    \n",
    "    return markdown_essay\n",
    "\n",
    "\n",
    "generate_markdown_essay(essay_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
