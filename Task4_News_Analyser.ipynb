{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from local_packages import *\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type = 'kimi'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '8k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat-5'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)\n",
    "\n",
    "js=JSProcessor()\n",
    "\n",
    "class ParseError(Exception):\n",
    "    def __init__(self, code, message=\"解析失败\"):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置工作目录\n",
    "期待输出的json能够在这个目录下遵循以下结构：\n",
    "- /root_folder\n",
    "    - raw_data.csv\n",
    "    - raw_data.json\n",
    "    - /step_1_results\n",
    "        - /step_1_processed_data\n",
    "            - result_data.json\n",
    "        - /step_1_unprocessed_data\n",
    "            - remain_data.json\n",
    "    - /step_2_results\n",
    "        - /step_2_unprocessed_data\n",
    "        - /step_2_mappings\n",
    "- model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置地址变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_folder='Task5_News_Analyser'\n",
    "\n",
    "step_1_processed_path=os.path.join(root_folder, 'step_1_results', 'step_1_processed_data')\n",
    "step_1_unprocessed_path=os.path.join(root_folder, 'step_1_results', 'step_1_unprocessed_data')\n",
    "step_2_processed_path=os.path.join(root_folder, 'step_2_results', 'step_2_processed_books')\n",
    "step_2_mapping_path=os.path.join(root_folder, 'step_2_results', 'step_2_mappings')\n",
    "\n",
    "# 模型和分词器的本地路径\n",
    "model_path = os.getenv('MODEL_PATH', None)\n",
    "\n",
    "#创建区：\n",
    "# 创建step_1_processed_path目录\n",
    "os.makedirs(step_1_processed_path, exist_ok=True)\n",
    "\n",
    "# 创建step_1_unprocessed_path目录\n",
    "os.makedirs(step_1_unprocessed_path, exist_ok=True)\n",
    "\n",
    "# 创建step_2_processed_path目录\n",
    "os.makedirs(step_2_processed_path, exist_ok=True)\n",
    "\n",
    "# 创建step_2_mapping_path目录\n",
    "os.makedirs(step_2_mapping_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换原始csv为json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_csv_and_convert_to_dict(file_path):\n",
    "    try:\n",
    "        # 读取CSV文件\n",
    "        df = pd.read_csv(file_path)\n",
    "        # 初始化一个空字典用来存储所有数据\n",
    "        df = df.applymap(lambda x: None if pd.isna(x) else x)\n",
    "        all_data = {}\n",
    "        # 遍历DataFrame的每一行\n",
    "        for index, row in df.iterrows():\n",
    "            # 生成每一行的字典，包括前17列和序号\n",
    "            row_dict = row[:17].to_dict()\n",
    "            row_dict['序号'] = index + 1  # 添加序号字段，从1开始\n",
    "            # 将这个行字典添加到总字典中，用序号作为键\n",
    "            all_data[f'行{index + 1}'] = row_dict\n",
    "        # 打印整个字典\n",
    "        print(all_data)\n",
    "        return all_data\n",
    "    except Exception as e:\n",
    "        print(\"读取CSV文件并转换为字典时出错:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path=os.path.join(root_folder,'raw_data.csv')\n",
    "json_path=os.path.join(root_folder,'raw_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **执行1_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dict=read_csv_and_convert_to_dict(csv_path)\n",
    "js.write_json(raw_dict,json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐条多线程处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Lock\n",
    "from queue import Queue, Empty\n",
    "import traceback\n",
    "\n",
    "# 签名注释用\n",
    "from typing import Generator\n",
    "\n",
    "# 定义一个线程安全的字典和锁\n",
    "result_dict = {}\n",
    "lock = Lock()\n",
    "\n",
    "#原子任务处理函数\n",
    "def parse_single_file(original_single_dict: dict, index:str, key_word: str) -> dict:\n",
    "    \"\"\"\n",
    "    解析单个文件，并返回解析结果字典。\n",
    "\n",
    "    参数:\n",
    "        original_single_dict (dict): 包含原始数据的字典。\n",
    "        target_keys (list): 目标键列表。\n",
    "\n",
    "    返回:\n",
    "        dict: 包含解析结果的字典，格式为 {书名: [知识点1, 知识点2, ...]}。\n",
    "\n",
    "    Raises:\n",
    "        ParseError: 当解析失败时引发自定义1001异常\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    comment=original_single_dict['微博正文']\n",
    "    # 构建提示信息\n",
    "    prompt = f'''\n",
    "    以下内容是{key_word}领域的一则微博评论{comment}，我要求你输出一个json字典，包含两个键值对，其键分别为“关键词”和“情感态度”，必须是如下结构：\n",
    "    {{\n",
    "    \"关键词\":[key_wd1, key_wd2, ...]\n",
    "    \"情感态度\":positive/negative/neutral\n",
    "    \"激发情感的原因\":...\n",
    "    }}\n",
    "    请注意，关键词对应的必须是列表，而情感态度对应的必须是以上三者中的一个；激发情感的原因对应一段中文解释\n",
    "    注意：务必精炼，省略一切无关内容\n",
    "    '''\n",
    "    try:\n",
    "        msg = service.ask_once(prompt)\n",
    "        if msg:\n",
    "            parse_success = js.parse_dict(msg)\n",
    "            if parse_success:\n",
    "                print(f\"成功解析：{index}\")\n",
    "                result = parse_success\n",
    "                with lock:\n",
    "                    result_dict[index] = result  # 将结果存入共享字典\n",
    "                print(result)\n",
    "            else:\n",
    "                # 解析失败，抛出异常\n",
    "                error_message = f\"解析失败：书名 {index}\"\n",
    "                error_code = 1001  # 自定义错误代码\n",
    "                raise ParseError(error_code, error_message)\n",
    "    except ParseError as e:\n",
    "        # 如果是解析失败的异常，直接向上抛出，不做处理\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        if 'Error code: 400' in str(e):\n",
    "            error_code = 400\n",
    "            error_message =\"发生400错误，跳过当前处理\"\n",
    "            return None\n",
    "        elif 'Error code: 429' in str(e):\n",
    "            error_code = 429\n",
    "            error_message =\"发生429错误，等待30秒后继续执行\"\n",
    "        else:\n",
    "            error_code = 1000\n",
    "            error_message =f\"发生未知错误{e}\"\n",
    "        raise ParseError(error_code, error_message)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "#多线程处理部分\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "\n",
    "# 工作线程函数\n",
    "def worker(task_queue):\n",
    "    while True:\n",
    "        try:\n",
    "            original_single_dict, index = task_queue.get(block=False)\n",
    "            parse_single_file(original_single_dict, index, '疫情健康码')\n",
    "            task_queue.task_done()\n",
    "        except Empty:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"未知错误: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            task_queue.task_done()\n",
    "\n",
    "# 多线程处理函数\n",
    "def multi_thread_processing(data_dict, num_threads=5):\n",
    "    task_queue = Queue()\n",
    "\n",
    "    # 将任务加入队列\n",
    "    for index, single_dict in data_dict.items():\n",
    "        task_queue.put((single_dict, index))\n",
    "\n",
    "    # 创建并启动线程\n",
    "    threads = []\n",
    "    for _ in range(num_threads):\n",
    "        thread = Thread(target=worker, args=(task_queue,))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # 等待队列清空\n",
    "    task_queue.join()\n",
    "\n",
    "    # 等待所有线程完成\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return result_dict  # 返回最终的结果字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **主执行块1_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = js.read_json(json_path)\n",
    "# 调用多线程处理函数，并打印结果\n",
    "final_results = multi_thread_processing(data_dict, num_threads=5)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果格式整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    合并两个字典，其中相同键的值（假设也是字典）也会合并。\n",
    "\n",
    "    参数:\n",
    "        dict1 (dict): 第一个字典。\n",
    "        dict2 (dict): 第二个字典。\n",
    "\n",
    "    返回:\n",
    "        dict: 合并后的字典。\n",
    "    \"\"\"\n",
    "    # 创建一个新字典来存储合并后的结果\n",
    "    merged_dict = {}\n",
    "    \n",
    "    # 获取所有键的集合\n",
    "    all_keys = set(dict1.keys()) | set(dict2.keys())\n",
    "    \n",
    "    # 遍历所有键，合并值\n",
    "    for key in all_keys:\n",
    "        if key in dict1 and key in dict2:\n",
    "            # 如果两个字典中都有这个键，合并这两个值（假设值也是字典）\n",
    "            merged_dict[key] = {**dict1[key], **dict2[key]}\n",
    "        elif key in dict1:\n",
    "            # 只在第一个字典中有这个键\n",
    "            merged_dict[key] = dict1[key]\n",
    "        else:\n",
    "            # 只在第二个字典中有这个键\n",
    "            merged_dict[key] = dict2[key]\n",
    "    \n",
    "    return merged_dict\n",
    "\n",
    "def sort_dict(data_dict):\n",
    "    sorted_keys = sorted(data_dict.keys(), key=lambda x: int(x[1:]))  # '行1', '行2', ... '行10'\n",
    "    \n",
    "    # 创建有序的字典，按照键的排序\n",
    "    sorted_dict = {key: data_dict[key] for key in sorted_keys}\n",
    "    return sorted_dict\n",
    "\n",
    "def rename_keys(data_dict):\n",
    "    \"\"\"\n",
    "    将原始字典的键转换为整数，并以这些整数作为新字典的键。\n",
    "\n",
    "    参数:\n",
    "        data_dict (dict): 原始的大字典，其中键是 '行n' 形式的字符串，n 为正整数。\n",
    "\n",
    "    返回:\n",
    "        dict: 包含新键的字典。\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for key, value in data_dict.items():\n",
    "        row_number = int(key[1:])  # 获取键中的行号，去掉 '行' 后转换为整数\n",
    "        new_dict[row_number] = value\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def dict_to_excel(data_dict, filename):\n",
    "    \"\"\"\n",
    "    将包含字典的大字典转换为Excel文件，先按键名排序。\n",
    "\n",
    "    参数:\n",
    "        data_dict (dict): 需要转换的大字典，其中每个元素都是一个小字典。\n",
    "        filename (str): 要保存的Excel文件的名称。\n",
    "    \"\"\"\n",
    "    # 对字典键进行排序，假设键的形式是 '行' 后跟一个数字\n",
    "    sorted_keys = sorted(data_dict.keys(), key=lambda x: int(x[1:]))  # '行1', '行2', ... '行10'\n",
    "    \n",
    "    # 创建有序的字典，按照键的排序\n",
    "    sorted_dict = {key: data_dict[key] for key in sorted_keys}\n",
    "    \n",
    "    # 将有序字典转换为DataFrame\n",
    "    df = pd.DataFrame.from_dict(sorted_dict, orient='index')\n",
    "    \n",
    "    # 将DataFrame保存为Excel文件\n",
    "    df.to_excel(filename, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **执行文件保存1_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=os.path.join(step_1_processed_path,'result.json')\n",
    "merged_result=merge_dicts(data_dict,final_results)\n",
    "sorted_dict=sort_dict(merged_result)\n",
    "rename_dict=rename_keys(sorted_dict)\n",
    "js.write_json(rename_dict,save_path)\n",
    "save_excel_file=os.path.join(step_1_processed_path,'result.xlsx')\n",
    "dict_to_excel(sorted_dict,'result.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、期待对数据结构做如下处理：\n",
    "- 建立序号为键，字典键值对为关键词 key:embedding 的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    从给定的路径加载模型和分词器。\n",
    "\n",
    "    参数:\n",
    "        model_path (str): 模型的路径。\n",
    "\n",
    "    返回:\n",
    "        tuple: 包含加载的模型和分词器。\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.eval()  # 确保模型处于评估模式\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(step_1_processed_path,'result.json')\n",
    "step_2_dict=js.read_json(file_path)\n",
    "for key, item in step_2_dict:\n",
    "    item['关键词']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立包含embedding的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def embed_and_export_dict_batch(given_dict, model_path, json_file_path, batch_size=32, stats_interval=5):\n",
    "    # 加载模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.eval()  # 确保模型处于评估模式\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 准备数据，以(key, item)的形式迭代\n",
    "    items = list(given_dict.items())\n",
    "    total_keys = len(items)\n",
    "    batches_processed = 0\n",
    "\n",
    "    for i in range(0, total_keys, batch_size):\n",
    "        batch_items = items[i:i+batch_size]\n",
    "        batch_key_names = [item[1][\"key_name\"] for item in batch_items]\n",
    "        inputs = tokenizer(batch_key_names, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "        # 将嵌入向量添加回原字典\n",
    "        for j, (key, _) in enumerate(batch_items):\n",
    "            given_dict[key][\"embedding\"] = embeddings[j].tolist()\n",
    "\n",
    "        batches_processed += 1\n",
    "\n",
    "        # 统计并打印当前速度\n",
    "        if batches_processed % stats_interval == 0 or (i + batch_size) >= total_keys:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            keys_processed = min((batches_processed) * batch_size, total_keys)\n",
    "            print(f\"已处理 {keys_processed}/{total_keys} 个key_name，耗时 {elapsed_time:.2f}秒，速度：{keys_processed / elapsed_time:.2f}个key_name/秒\")\n",
    "\n",
    "    # 全部处理完毕后一次性保存到JSON文件\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(given_dict, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"完成，已将更新后的字典导出到指定的JSON文件中。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **执行块3_3**\n",
    "在这里我们得到包含着embedding的键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_dict=mapping_dict\n",
    "json_file_path=os.path.join(step_2_mapping_path,'mapping_embedding_dict.json')\n",
    "embed_and_export_dict_batch(given_dict,model_path,json_file_path,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐层比对融合\n",
    "还回末端字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_null_values_back(original_dict, given_dict, current_path=[], export_folder=None):\n",
    "    \"\"\"\n",
    "    递归地向给定字典中添加原本为null的键值对。\n",
    "    \n",
    "    :param original_dict: 原始的字典。\n",
    "    :param given_dict: 经过处理的字典，需要在这个字典中添加值。\n",
    "    :param current_path: 当前的遍历路径，用于构建给定字典中的键。\n",
    "    :param export_folder: 导出文件夹的路径。\n",
    "    \"\"\"\n",
    "    for key, value in original_dict.items():\n",
    "        # 更新当前路径\n",
    "        new_path = current_path + [key]\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            # 检查是否是最底层的dict\n",
    "            if all(v is None for v in value.values()):\n",
    "                # 如果是最底层的dict，构建在given_dict中对应的键\n",
    "                given_dict_key = \"_\".join(new_path)\n",
    "                # 在given_dict中找到对应的项并添加\"value\"键\n",
    "                given_dict_item = given_dict.get(given_dict_key)\n",
    "                if given_dict_item is not None:\n",
    "                    given_dict_item['value'] = {sub_key: None for sub_key in value}\n",
    "            else:\n",
    "                # 如果不是最底层的dict，继续递归遍历\n",
    "                add_null_values_back(value, given_dict, new_path, export_folder=None)\n",
    "                \n",
    "    # 导出结果到文件\n",
    "    if export_folder:\n",
    "        export_file_path = os.path.join(export_folder, \"mapping_with_embedding_value.json\")\n",
    "        with open(export_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(given_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **执行块3_4** 存入指定文件'mapping_embedding_dict.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_processed_json_path=os.path.join(step_2_processed_path,'transformed_results.json')\n",
    "json_file_path=os.path.join(step_2_mapping_path,'mapping_embedding_dict.json')\n",
    "original_dict=js.read_json(step_2_processed_json_path)\n",
    "given_dict=js.read_json(json_file_path)\n",
    "export_folder=step_2_mapping_path\n",
    "# 示例使用\n",
    "add_null_values_back(original_dict, given_dict,export_folder=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐层深入，迭代构建并聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_matrix(vectors):\n",
    "    \"\"\"构建余弦相似度矩阵\"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # 避免除以零\n",
    "    normalized_vectors = vectors / norms\n",
    "    similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "def build_mapping_relation(data, depth, similarity_threshold=0.8):\n",
    "    groups = {}\n",
    "    # 分组\n",
    "    for key, item in data.items():\n",
    "        if item['depth'] == depth:  # 只处理depth=3的键\n",
    "            prefix = \"_\".join(key.split(\"_\")[:3])\n",
    "            groups.setdefault(prefix, []).append(key)\n",
    "\n",
    "    # 初始化映射关系为字典\n",
    "    mapping_relation = {}\n",
    "\n",
    "    for group, keys in groups.items():\n",
    "        if len(keys) < 2:  # 单个键无需比较\n",
    "            continue\n",
    "        # 构建矩阵\n",
    "        vectors = np.array([data[key]['embedding'] for key in keys])\n",
    "        # 计算余弦相似度矩阵\n",
    "        sim_matrix = cosine_similarity_matrix(vectors)\n",
    "        \n",
    "        for i, key in enumerate(keys):\n",
    "            # 找到与当前键相似度超过阈值的所有键\n",
    "            sim_indices = np.where(sim_matrix[i] > similarity_threshold)[0]\n",
    "            sim_indices = sim_indices[sim_indices != i]  # 排除自己\n",
    "            if len(sim_indices) == 0:\n",
    "                continue  # 没有超过阈值的相似键，跳过\n",
    "\n",
    "            # 获取相似度超过阈值的键列表\n",
    "            similar_keys = [keys[j] for j in sim_indices]\n",
    "            # 检查当前键是否已经作为其他键的相似项被并入\n",
    "            if not any(key in v for v in mapping_relation.values()):\n",
    "                mapping_relation[key] = similar_keys\n",
    "\n",
    "    return mapping_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_embeddings(data,depth):\n",
    "    \"\"\"\n",
    "    移除depth小于3的键的embedding属性。\n",
    "    \"\"\"\n",
    "    for key, item in data.items():\n",
    "        if item['depth'] < depth and 'embedding' in item:\n",
    "            del item['embedding']\n",
    "\n",
    "def merge_dicts(data, mapping_relation):\n",
    "    \"\"\"\n",
    "    根据并射集合并字典项。\n",
    "    \"\"\"\n",
    "    for target_key, source_keys in mapping_relation.items():\n",
    "        for source_key in source_keys:\n",
    "            source_item = data.get(source_key)\n",
    "            target_item = data.get(target_key)\n",
    "            if source_item and target_item:\n",
    "                if 'value' in target_item:\n",
    "                    target_item['value'].update(source_item.get('value', {}))\n",
    "                else:\n",
    "                    target_item['value'] = source_item.get('value', {})\n",
    "\n",
    "def update_keys(data, mapping_relation):\n",
    "    \"\"\"\n",
    "    更新字典中的键路径。\n",
    "    \"\"\"\n",
    "    # 反向构建路径替换映射\n",
    "    replacement_map = {source_key: target_key for target_key, source_keys in mapping_relation.items() for source_key in source_keys}\n",
    "    # 更新数据\n",
    "    new_data = {}\n",
    "    for key, item in data.items():\n",
    "        # 检查当前键是否需要被替换\n",
    "        new_key = replacement_map.get(key, key)\n",
    "        new_data[new_key] = item\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_data(data, start_depth=3, similarity_threshold=0.8):\n",
    "    max_depth = max(item['depth'] for item in data.values())  # 获取最大深度\n",
    "    for depth in range(start_depth, max_depth + 1):\n",
    "        # 步骤1: 构建映射关系\n",
    "        mapping_relation = build_mapping_relation(data, depth, similarity_threshold)\n",
    "        \n",
    "        # 步骤2: 移除embedding属性\n",
    "        remove_embeddings(data, depth)\n",
    "        \n",
    "        # 步骤3: 合并字典项\n",
    "        merge_dicts(data, mapping_relation)\n",
    "        \n",
    "        # 步骤4: 更新键路径\n",
    "        data = update_keys(data, mapping_relation)\n",
    "    for k,v in data.items():\n",
    "        if 'embedding' in v:\n",
    "            del v['embedding']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **执行块3_5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=js.read_json(os.path.join(step_2_mapping_path,'mapping_with_embedding_value.json'))\n",
    "new_data=process_data(data)\n",
    "output_path=os.path.join(step_2_mapping_path,'aggregated_dict.json')\n",
    "js.write_json(new_data,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=os.path.join(step_2_mapping_path,'aggregated_dict.json')\n",
    "data=js.read_json(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重建树形结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_tree_structure(flat_dict):\n",
    "    root = {}\n",
    "\n",
    "    # 遍历每个键值对构建树\n",
    "    for full_path, item in flat_dict.items():\n",
    "        # 分割路径\n",
    "        parts = full_path.split(\"_\")\n",
    "        current_level = root\n",
    "\n",
    "        # 遍历路径的每一部分，逐层深入\n",
    "        for part in parts[:-1]:\n",
    "            # 如果当前层级还没有这个部分的键，则创建一个新的字典\n",
    "            if part not in current_level:\n",
    "                current_level[part] = {}\n",
    "            current_level = current_level[part]\n",
    "\n",
    "        # 对于value字段，需要特别处理\n",
    "        if 'value' in item and item['value']:\n",
    "            # 如果当前节点下有value，则将其作为当前节点的子节点\n",
    "            current_level[parts[-1]] = {k: None for k in item['value']}\n",
    "        else:\n",
    "            # 如果没有value字段，或者value为空，则直接将该节点置为null\n",
    "            current_level[parts[-1]] = None\n",
    "\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_tree(dictionary):\n",
    "    tree = {}\n",
    "    for key, value in dictionary.items():\n",
    "        keys = key.split('_')\n",
    "        current_level = tree\n",
    "        for k in keys:\n",
    "            if k not in current_level:\n",
    "                current_level[k] = {}\n",
    "            current_level = current_level[k]\n",
    "        # 确保value是一个字典且包含'value'键，然后更新current_level\n",
    "        if isinstance(value, dict) and \"value\" in value:\n",
    "            # 确保current_level是一个字典\n",
    "            if isinstance(current_level, dict):\n",
    "                current_level.update(value[\"value\"])\n",
    "            else:\n",
    "                # 如果current_level不是字典，这可能是逻辑上的错误\n",
    "                print(f\"Unexpected type for current_level: {type(current_level)}\",k)\n",
    "    \n",
    "    # 遍历树，将空字典替换为None\n",
    "    def replace_empty_with_none(node):\n",
    "        for k, v in node.items():\n",
    "            if isinstance(v, dict) and not v:  # 空字典 {}\n",
    "                node[k] = None\n",
    "            elif isinstance(v, dict):\n",
    "                replace_empty_with_none(v)\n",
    "    \n",
    "    replace_empty_with_none(tree)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = rebuild_tree(data)\n",
    "js.write_json(tree,'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dict_to_hierarchy(dictionary, depth=0):\n",
    "    result = ''\n",
    "    for key, value in dictionary.items():\n",
    "        result += '  ' * depth + '- ' + key + '\\n'\n",
    "        if isinstance(value, dict):\n",
    "            result += format_dict_to_hierarchy(value, depth + 1)\n",
    "    return result\n",
    "\n",
    "def write_hierarchy_to_file(formatted_hierarchy, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_hierarchy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
