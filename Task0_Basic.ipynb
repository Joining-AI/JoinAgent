{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from local_packages import *\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type ='qwen'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['qwen']:\n",
    "        version = 'long'\n",
    "        service=QwenService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '32k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['deepseek']:\n",
    "        version = 'chat'\n",
    "        service = DeepSeekService(version)\n",
    "    elif service_type in ['huida']:\n",
    "        version = 'gpt-4o'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = HuidaService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)\n",
    "\n",
    "js=JSProcessor()\n",
    "\n",
    "class ParseError(Exception):\n",
    "    def __init__(self, code, message=\"解析失败\"):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.ask_once(\"你谁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from threading import Thread\n",
    "from pyngrok import ngrok\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 处理函数示例：反转字符串\n",
    "def process_string(data_str):\n",
    "    return data_str[::-1]\n",
    "\n",
    "@app.route('/process', methods=['POST'])\n",
    "def process():\n",
    "    data = request.json\n",
    "    data_str = data.get('data_str')\n",
    "    if data_str:\n",
    "        processed_str = process_string(data_str)\n",
    "        return jsonify({\"processed_str\": processed_str})\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No data_str provided\"}), 400\n",
    "\n",
    "def run_app():\n",
    "    app.run()\n",
    "\n",
    "# 使用线程在后台运行Flask应用\n",
    "thread = Thread(target=run_app)\n",
    "thread.start()\n",
    "\n",
    "# 使用ngrok将本地服务器暴露到公网\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f\"Public URL: {public_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 定义根文件夹和模型路径\n",
    "root_folder = 'Task0_0622_MCM_KG_analyser'\n",
    "model_path = r'D:\\Joining\\Models\\Text2Vec_base_zh'\n",
    "\n",
    "pdf_file_path=os.path.join(root_folder,'pdf_files.json')\n",
    "\n",
    "print(\"所有目录和文件已成功创建。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取知识库的各级目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "menu_structure_path=os.path.join(root_folder,\"menu_structure.json\")\n",
    "# 假设从 JSON 文件中读取数据\n",
    "mcm_kg_base = js.read_json(menu_structure_path)\n",
    "\n",
    "def extract_keys(data, level=0, parent_key=\"\"):\n",
    "    keys_dict = {}\n",
    "\n",
    "    def inner_extract(data, level, parent_key):\n",
    "        if level not in keys_dict:\n",
    "            keys_dict[level] = []\n",
    "\n",
    "        for key, value in data.items():\n",
    "            current_key = parent_key + key if parent_key else key\n",
    "            keys_dict[level].append(current_key)\n",
    "\n",
    "            if isinstance(value, dict):\n",
    "                inner_extract(value, level + 1, current_key + \" > \")\n",
    "\n",
    "    inner_extract(data, level, parent_key)\n",
    "    return keys_dict\n",
    "\n",
    "keys_dict = extract_keys(mcm_kg_base[\"模型\"])\n",
    "\n",
    "for level in sorted(keys_dict.keys()):\n",
    "    print(f\"Level {level}:\")\n",
    "    for key in keys_dict[level]:\n",
    "        print(f\"  {key}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取优秀论文目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def list_pdf_files(directory):\n",
    "    pdf_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                pdf_files.append(os.path.join(root, file))\n",
    "    return pdf_files\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def main():\n",
    "    directory = \"D:\\\\CUMCM2024\\\\CUMCM_2012_2023_Best_Papers\"\n",
    "    pdf_files = list_pdf_files(directory)\n",
    "    save_to_json(pdf_files, 'pdf_files.json')\n",
    "    print(f\"PDF文件路径已保存到 pdf_files.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_list=js.read_json(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "\n",
    "\n",
    "# 处理单个 PDF 文件的函数，包含指数退避策略\n",
    "def process_pdf(file_path):\n",
    "    max_retries = 3\n",
    "    retry_delay = 10  # 初始延迟时间\n",
    "    prompt_1 = f'''\n",
    "    请你查看输入的这份pdf论文，给出它涉及到的主要学科知识点，用键值对的形式表示，其键必须为[\"知识点xx\"]，值为知识点，每一条知识点可能有多个层级，各层之间用-连接，例如:是lv1-lv2-lv3的格式，以json dict的格式输出，例如:\n",
    "    {{\n",
    "        \"知识点1\":\"控制论-控制技巧-根轨迹分析\",\n",
    "        \"知识点2\":\"最优化方法-约束优化\",\n",
    "        \"知识点3\":\"农林环材\",\n",
    "        ...\n",
    "    }}\n",
    "    请你注意，lv1必须出自以下这个列表的主题中:[运筹学,  最优化方法,  机器学习,  计算与模拟,  概率统计,  经济管理,  预测主题,  评价主题,  数据科学,  物理学,  生物医学,  物理化学,  信号理论,  农林环材,  控制论]\n",
    "不许自由发挥\n",
    "    '''\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            answer = service.chat_with_file(file_path, prompt_1)\n",
    "            if any(error in answer for error in ['invalid_parameter_error', 'RequestTimeOut']) and 'please try again later' in answer:\n",
    "                raise Exception('File parsing in progress or request timed out, please try again later.')\n",
    "            answer_dict = js.parse_dict(answer)\n",
    "            answer_dict[\"文件目录\"] = file_path\n",
    "            print(answer_dict)\n",
    "            return answer_dict\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if 'rate_limit_reached_error' in error_message or 'File parsing in progress' in error_message or 'RequestTimeOut' in error_message:\n",
    "                print(f\"Rate limit reached or file parsing/request timeout for {file_path}. Retrying in {retry_delay} seconds.\")\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # 指数增加延迟时间\n",
    "            else:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "                return None\n",
    "    \n",
    "    print(f\"Failed to process {file_path} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# 主函数\n",
    "def main(pdf_file_path):\n",
    "    file_path_list = js.read_json(pdf_file_path)\n",
    "    dict_list = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        futures = {executor.submit(process_pdf, file_path): file_path for file_path in file_path_list}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                dict_list.append(result)\n",
    "    return dict_list\n",
    "dict_list = main(pdf_file_path)\n",
    "list_path=os.path.join(root_folder,\"knowledge_dict_list.json\")\n",
    "js.write_json(dict_list,list_path)\n",
    "print(dict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件内容重建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_sub_dict(sub_dict, service, js):\n",
    "    json_file_path = sub_dict[\"文件目录\"]\n",
    "    parts = {}\n",
    "    for key, value in sub_dict.items():\n",
    "        if key.startswith(\"方法\"):\n",
    "            parts[key] = value\n",
    "    results = []\n",
    "    for part_key, part_value in parts.items():\n",
    "        prompt_1 = f'''\n",
    "        请你从这篇pdf论文中，找到这个数模方法{part_value}对应的完整建模与使用过程,对应解释并全部输出，其中的数学公式使用latex语法，要求你输出的格式是json dict格式：\n",
    "        {{\n",
    "            \"{part_value}\":\"original_markdown_str...\"            \n",
    "        }}\n",
    "        请original_markdown_str尽量调整得在一行，尽量不换行\n",
    "        '''\n",
    "        prompt_2 = f'''\n",
    "        请你从这篇pdf论文中，找到这个数模方法{part_value}对应的完整建模与使用过程,对应解释并全部输出，不许添加任何无关内容，第一个字就必须是此方法在原文中的对应第一个字\n",
    "        '''\n",
    "        attempt = 0\n",
    "        while attempt < 3:  # 最多重试3次\n",
    "            try:\n",
    "                prompt = prompt_1 if attempt == 0 else prompt_2\n",
    "                answer = service.chat_with_file(json_file_path, prompt)\n",
    "                \n",
    "                if any(error in answer for error in ['invalid_parameter_error', 'RequestTimeOut']) and 'please try again later' in answer:\n",
    "                    raise Exception('File parsing in progress or request timed out, please try again later.')\n",
    "                \n",
    "                if attempt == 0:\n",
    "                    answer_dict = js.parse_dict(answer)\n",
    "                else:\n",
    "                    # 第二次尝试的处理逻辑\n",
    "                    answer_dict = {part_value: answer}\n",
    "                answer_dict[\"文件目录\"] = json_file_path\n",
    "                print(json_file_path, part_value, answer_dict, attempt)\n",
    "                results.append(answer_dict)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if any(error in str(e) for error in ['rate_limit_reached_error', 'File parsing in progress', 'Request timed out']):\n",
    "                    wait_time = (2 ** attempt) * 10  # 指数退避策略\n",
    "                    print(f\"处理 {json_file_path} 中的 {part_value} 时遇到速率限制、文件解析未完成或请求超时，等待 {wait_time} 秒后重试...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    attempt += 1\n",
    "                else:\n",
    "                    print(f\"处理 {json_file_path} 中的 {part_value} 时出错: {e}\")\n",
    "                    if attempt == 0:\n",
    "                        print(f\"尝试使用替代提示词和处理逻辑处理 {json_file_path} 中的 {part_value} ...\")\n",
    "                        attempt += 1\n",
    "                    else:\n",
    "                        break\n",
    "    return results\n",
    "\n",
    "def merge_dicts(dicts):\n",
    "    merged = {}\n",
    "    for d in dicts:\n",
    "        file_path = d.pop(\"文件目录\")\n",
    "        if file_path not in merged:\n",
    "            merged[file_path] = {}\n",
    "        merged[file_path].update(d)\n",
    "    return merged\n",
    "\n",
    "def read_and_process_json(file_path, service, js):\n",
    "    dict_list = js.read_json(file_path)\n",
    "\n",
    "    # 合并相同文件目录的子字典\n",
    "    combined_dicts = {}\n",
    "    for sub_dict in dict_list:\n",
    "        file_path = sub_dict[\"文件目录\"]\n",
    "        if file_path not in combined_dicts:\n",
    "            combined_dicts[file_path] = {}\n",
    "        combined_dicts[file_path].update(sub_dict)\n",
    "\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=40) as executor:\n",
    "        future_to_sub_dict = {executor.submit(process_sub_dict, sub_dict, service, js): sub_dict for sub_dict in combined_dicts.values()}\n",
    "        for future in as_completed(future_to_sub_dict):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.extend(result)\n",
    "            except Exception as e:\n",
    "                print(f\"处理子字典时出错: {e}\")\n",
    "\n",
    "    merged_results = merge_dicts(results)\n",
    "    return merged_results\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"D:\\\\Joining\\\\Joining-Agents0622\\\\Task0_0622_MCM_KG_analyser\\\\method_dict_list.json\"\n",
    "\n",
    "# 调用函数并获取结果\n",
    "merged_results = read_and_process_json(file_path, service, js)\n",
    "\n",
    "# 打印最终结果\n",
    "print(json.dumps(merged_results, ensure_ascii=False, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js.write_json(merged_results,'detailed_method_dict_list')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
