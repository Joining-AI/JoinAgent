{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from LLM_API import GLMService, SenseService, KimiService\n",
    "from Json_Processor import JSProcessor\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type = 'kimi'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '8k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)\n",
    "\n",
    "js=JSProcessor()\n",
    "\n",
    "class ParseError(Exception):\n",
    "    def __init__(self, code, message=\"解析失败\"):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置工作目录\n",
    "期待输出的json能够在这个目录下遵循以下结构：\n",
    "- /root_folder\n",
    "    - Raw_Books.json\n",
    "    - /step_1_books\n",
    "        - /step_1_processed_books\n",
    "            - lv1_lv2_lv3_results.json\n",
    "        - /step_1_unprocessed_books\n",
    "            - Remain_Books.json\n",
    "            - /json_lists\n",
    "                - lv1_unprocessed_books.json\n",
    "    - /step_2_books\n",
    "        - /step_2_processed_books\n",
    "            - transformed_results.json\n",
    "        - /step_2_mappings\n",
    "            - mapping_dict.json\n",
    "            - mapping_embedding_dict.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置地址变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder='Edu_KG'\n",
    "\n",
    "step_1_processed_path=os.path.join(root_folder, 'step_1_books', 'step_1_processed_books')\n",
    "step_1_unprocessed_path=os.path.join(root_folder, 'step_1_books', 'step_1_unprocessed_books','json_lists')\n",
    "step_2_processed_path=os.path.join(root_folder, 'step_2_books', 'step_2_processed_books')\n",
    "step_2_mapping_path=os.path.join(root_folder, 'step_2_books', 'step_2_mappings')\n",
    "\n",
    "# 模型和分词器的本地路径\n",
    "model_path = \"D:\\Joining\\Models\\Text2Vec_base_zh\"\n",
    "catalog_file_path = os.path.join(root_folder, 'Raw_Books.json')\n",
    "step_1_remain_path=os.path.join(root_folder, 'step_1_books', 'step_1_unprocessed_books','Remain_Books.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 书本目录处理为知识点\n",
    "\n",
    "现在不会覆写了\n",
    "写了健全的多线程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Lock\n",
    "from queue import Queue, Empty\n",
    "\n",
    "import traceback\n",
    "import urllib.error\n",
    "#签名注释用\n",
    "from typing import Generator\n",
    "\n",
    "#原子任务处理函数\n",
    "def parse_single_file(original_single_dict: dict, target_keys: list) -> dict:\n",
    "    \"\"\"\n",
    "    解析单个文件，并返回解析结果字典。\n",
    "\n",
    "    参数:\n",
    "        original_single_dict (dict): 包含原始数据的字典。\n",
    "        target_keys (list): 目标键列表。\n",
    "\n",
    "    返回:\n",
    "        dict: 包含解析结果的字典，格式为 {书名: [知识点1, 知识点2, ...]}。\n",
    "\n",
    "    Raises:\n",
    "        ParseError: 当解析失败时引发自定义1001异常\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    # 提取书名和目录信息\n",
    "    book_name = str(original_single_dict.get('书名', '未提供'))\n",
    "    catalog = str(original_single_dict.get('目录', '未提供'))\n",
    "\n",
    "    # 构建提示信息\n",
    "    prompt = f'''\n",
    "    以下内容是{target_keys[-1]}领域的书籍目录，书名{book_name}，目录内容为：{catalog}，我要求你输出一个python列表，其中的值是知识点，必须是如下结构的一个python列表，由中括号所表示：\n",
    "    ['知识点1','知识点2','知识点3',...]\n",
    "    注意：务必精炼，省略一切无关内容\n",
    "    '''\n",
    "    try:\n",
    "        msg = service.ask_once(prompt)\n",
    "        if msg:\n",
    "            parse_success = js.parse_list(msg)\n",
    "            if parse_success:\n",
    "                print(f\"成功解析：{book_name}\")\n",
    "                result[book_name] = parse_success\n",
    "            else:\n",
    "                # 解析失败，抛出异常\n",
    "                error_message = f\"解析失败：书名 {book_name}\"\n",
    "                error_code = 1001  # 自定义错误代码\n",
    "                raise ParseError(error_code, error_message)\n",
    "    except ParseError as e:\n",
    "        # 如果是解析失败的异常，直接向上抛出，不做处理\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        if 'Error code: 400' in str(e):\n",
    "            error_code = 400\n",
    "            error_message =\"发生400错误，跳过当前处理\"\n",
    "            return None\n",
    "        elif 'Error code: 429' in str(e):\n",
    "            error_code = 429\n",
    "            error_message =\"发生429错误，等待30秒后继续执行\"\n",
    "        else:\n",
    "            error_code = 1000\n",
    "            error_message =f\"发生未知错误{e}\"\n",
    "        raise ParseError(error_code, error_message)\n",
    "        \n",
    "    return result\n",
    "# 辅助函数：收集字典键的路径\n",
    "def collect_key_paths(current_dict: dict, current_path: list = []) -> Generator[list, None, None]:\n",
    "    \"\"\"\n",
    "    递归收集字典键的路径。\n",
    "\n",
    "    参数:\n",
    "        current_dict (dict): 当前正在遍历的字典对象。\n",
    "        current_path (list, 可选): 当前的键路径。默认为一个空列表。\n",
    "\n",
    "    返回:\n",
    "        生成器: 生成当前字典中所有键的路径。\n",
    "        路径列表：[lv1, lv2, lv3..., leaf]\n",
    "\n",
    "    注意:\n",
    "        该函数用于递归遍历字典对象，并生成所有键的路径。\n",
    "        返回的生成器会逐个生成路径列表，每个列表表示从字典的根节点到叶子节点的路径。\n",
    "    \"\"\"\n",
    "    if isinstance(current_dict, dict):  # 确保当前对象是字典\n",
    "        for key, value in current_dict.items():\n",
    "            new_path = current_path + [key]\n",
    "            if isinstance(value, dict):  # 如果值也是字典，则继续递归\n",
    "                yield from collect_key_paths(value, new_path)\n",
    "            else:\n",
    "                yield new_path\n",
    "    else:\n",
    "        yield current_path\n",
    "\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "#多线程处理部分\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "#任务生成函数\n",
    "def task_generator(original_dict: dict, target_categories: list):\n",
    "    \"\"\"\n",
    "    根据目标类别列表生成待处理的任务。\n",
    "    \n",
    "    参数:\n",
    "        original_dict (dict): 包含原始数据的字典。\n",
    "        target_categories (list): 目标类别列表，只包含希望处理的第一层键。\n",
    "\n",
    "    返回:\n",
    "        Generator: 生成包含任务元组的生成器，每个元组包含一个字典和对应的目标键前三层。\n",
    "    \"\"\"\n",
    "    for first_level_key in original_dict:\n",
    "        if first_level_key in target_categories:  # 检查第一层键是否在目标范围内\n",
    "            sub_dict = original_dict[first_level_key]\n",
    "            for second_level_key in sub_dict:\n",
    "                for third_level_key in sub_dict[second_level_key]:\n",
    "                    item_list = sub_dict[second_level_key][third_level_key]\n",
    "                    if isinstance(item_list, list) and item_list:  # 确保是非空列表\n",
    "                        for item_dict in item_list:\n",
    "                            if isinstance(item_dict, dict) and item_dict:  # 确保是非空字典\n",
    "                                # 提取前三层键作为目标键，并加上当前非空字典\n",
    "                                target_keys = [first_level_key, second_level_key, third_level_key]\n",
    "                                yield (item_dict, target_keys)\n",
    "\n",
    "#单线程工作函数\n",
    "def worker(task_queue: Queue, result_dict: dict, lock: Lock):\n",
    "    \"\"\"\n",
    "    工作线程函数，从任务队列中获取任务并处理。\n",
    "\n",
    "    参数:\n",
    "        task_queue (Queue): 任务队列，存储待处理的任务。\n",
    "        result_dict (dict): 存储解析结果的字典。\n",
    "        lock (Lock): 线程锁，用于确保结果字典的安全访问。\n",
    "\n",
    "    返回:\n",
    "        无。\n",
    "\n",
    "    注意:\n",
    "        该函数循环从任务队列中获取任务，并处理解析过程。\n",
    "        每个任务包括一个原始单个字典和目标键列表。\n",
    "        在最多允许重试两次的情况下，尝试解析原始单个字典，并将结果存储到结果字典中。\n",
    "        解析失败时，根据错误代码采取相应的操作，例如重试、休眠等。\n",
    "        最后，标记任务完成并释放锁，确保线程安全操作。\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        task_acquired = False  # 添加标志变量\n",
    "        try:\n",
    "            original_single_dict, target_keys = task_queue.get(block=False)\n",
    "            task_acquired = True  # 成功获取任务后，设置标志变量为True\n",
    "            retry_count = 0\n",
    "            while retry_count <= 2:  # 允许最多重试两次\n",
    "                try:\n",
    "                    parsed_result = parse_single_file(original_single_dict, target_keys)\n",
    "                    with lock:\n",
    "                        # 按层级结构存储解析结果\n",
    "                        current_dict = result_dict\n",
    "                        for key in target_keys[:-1]:  # 遍历至倒数第二个键，逐层深入\n",
    "                            current_dict = current_dict.setdefault(key, {})\n",
    "                        current_dict[target_keys[-1]] = {**current_dict.get(target_keys[-1], {}), **parsed_result}\n",
    "                    break  # 解析成功，跳出重试循环\n",
    "                \n",
    "                except ParseError as e:\n",
    "                    if e.code == 1001:\n",
    "                        retry_count += 1\n",
    "                        if retry_count > 2:\n",
    "                            print(f\"解析错误: {e.message}, 已达最大重试次数\")\n",
    "                            break\n",
    "                    elif e.code == 429:\n",
    "                        time.sleep(20)  # 休眠20秒后重试\n",
    "                    elif e.code == 401 or e.code not in [1001, 429]:\n",
    "                        print(f\"解析错误: {e.message}, 跳过\")\n",
    "                        break\n",
    "        except Empty:  # 使用正确的异常类型\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"未预料的错误: {traceback.format_exc()}\")\n",
    "        finally:\n",
    "            if task_acquired:  # 只有在成功获取任务后才调用task_done()\n",
    "                task_queue.task_done()\n",
    "\n",
    "#这个函数很需要扔到类里面\n",
    "def save_to_file(folder_path, file_name, data):\n",
    "    \"\"\"将数据增量保存到指定的文件中。\"\"\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    try:\n",
    "        # 尝试读取现有数据\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                existing_data = json.load(file)\n",
    "        else:\n",
    "            existing_data = {}\n",
    "    except json.JSONDecodeError:\n",
    "        # 如果文件内容不是有效的JSON，初始化为空字典\n",
    "        existing_data = {}\n",
    "\n",
    "    # 合并数据\n",
    "    merged_data = {**existing_data, **data}\n",
    "\n",
    "    # 写回文件\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(merged_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "#执行处理部分\n",
    "#—————————————————————————————————————————————————————————————————————————————————————\n",
    "#执行函数\n",
    "def multi_thread_parse(original_dict: dict, target_categories: list, thread_number: int, folder_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    多线程解析函数，解析原始字典中的数据，并将结果保存到文件。\n",
    "\n",
    "    参数:\n",
    "        original_dict (dict): 包含原始数据的字典。\n",
    "        target_categories (list): 目标类别列表，包含希望处理的前三层键。\n",
    "        thread_number (int): 线程数量，用于并行处理任务。\n",
    "        folder_path (str): 结果文件夹路径，用于保存解析结果文件。\n",
    "\n",
    "    返回:\n",
    "        dict: 包含解析结果的字典。\n",
    "\n",
    "    注意:\n",
    "        该函数使用多线程并行处理任务，解析原始字典中的数据。\n",
    "        首先，生成任务并放入任务队列中。\n",
    "        然后，创建并启动指定数量的工作线程，每个线程从队列中获取任务并处理。\n",
    "        等待所有任务完成后，保存结果到指定的文件夹中。\n",
    "        最后，返回包含解析结果的字典。\n",
    "    \"\"\"\n",
    "    task_queue = Queue()\n",
    "    result_dict = {}\n",
    "    lock = Lock()\n",
    "\n",
    "    # 生成任务并放入队列\n",
    "    for task in task_generator(original_dict, target_categories):\n",
    "        task_queue.put(task)\n",
    "\n",
    "    # 创建并启动线程\n",
    "    threads = []\n",
    "    for _ in range(thread_number):  # 可以根据实际情况调整线程数\n",
    "        thread = Thread(target=worker, args=(task_queue, result_dict, lock))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # 等待所有任务完成\n",
    "    task_queue.join()\n",
    "\n",
    "    # 等待所有线程结束\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    file_name = 'step_1_processed.json'\n",
    "    # 保存结果到文件\n",
    "    save_to_file(folder_path, file_name, result_dict)\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **主处理功能块1：**\n",
    "- /Edu_KG\n",
    "    - Raw_Books.json\n",
    "    - /step_1_books\n",
    "        - /step_1_processed_books\n",
    "            - step_1_processed.json\n",
    "        - /step_1_unprocessed_books\n",
    "            - Remain_Books.json\n",
    "            - /json_lists\n",
    "                - lv1_unprocessed_books.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **执行功能块1_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dict=js.read_json(step_1_remain_path)\n",
    "target_categories=[]\n",
    "for key, item in original_dict.items():\n",
    "    target_categories.append(key)\n",
    "\n",
    "multi_thread_parse(original_dict,target_categories,thread_number=50,folder_path=step_1_processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  对比处理完成量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_original_dict(d, simplified=None):\n",
    "    \"\"\"递归简化original_dict的结构\"\"\"\n",
    "\n",
    "    if simplified is None:\n",
    "        simplified = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, list):\n",
    "            # 假设列表中的元素都是包含'书名'键的字典\n",
    "            simplified[k] = [{'书名': book['书名']} for book in v]\n",
    "        elif isinstance(v, dict):\n",
    "            simplified[k] = simplify_original_dict(v, {})\n",
    "    return simplified\n",
    "def simplify_processed_dict(d, simplified=None):\n",
    "    \"\"\"递归简化processed_dict的结构\"\"\"\n",
    "    if simplified is None:\n",
    "        simplified = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            if all(isinstance(val, list) for val in v.values()):\n",
    "                # 假设如果字典的所有值都是列表，则这是lv3层\n",
    "                simplified[k] = {sub_k: None for sub_k in v}\n",
    "            else:\n",
    "                simplified[k] = simplify_processed_dict(v, {})\n",
    "    return simplified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_dicts(original_dict, processed_dict, count_dict=None):\n",
    "    \"\"\"\n",
    "    从original_dict中减去processed_dict中的内容，返回结果字典。\n",
    "    同时，通过count_dict参数统计原始、处理后(减去的部分)、最后剩下的书的数量。\n",
    "    \"\"\"\n",
    "    if count_dict is None:\n",
    "        count_dict = {'original': 0, 'processed': 0, 'remaining': 0}\n",
    "\n",
    "    result_dict = {}\n",
    "    for key, value in original_dict.items():\n",
    "        if key not in processed_dict:\n",
    "            result_dict[key] = value\n",
    "            if isinstance(value, list):\n",
    "                count_dict['original'] += len(value)\n",
    "                count_dict['remaining'] += len(value)\n",
    "        elif isinstance(value, dict):\n",
    "            result_dict[key] = subtract_dicts(value, processed_dict.get(key, {}), count_dict)\n",
    "        elif isinstance(value, list) and all(isinstance(item, dict) for item in value):\n",
    "            result_list = []\n",
    "            processed_books = processed_dict.get(key, {})\n",
    "            for book in value:\n",
    "                if \"书名\" in book and book[\"书名\"] not in processed_books:\n",
    "                    result_list.append(book)\n",
    "                    count_dict['remaining'] += 1\n",
    "                else:\n",
    "                    count_dict['processed'] += 1\n",
    "            if result_list:\n",
    "                result_dict[key] = result_list\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "def find_unprocessed_books(original_json_path, step_1_processed_path, export_json_folder, top_level_keys):\n",
    "    results = {}\n",
    "\n",
    "    with open(original_json_path, 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "    \n",
    "    with open(step_1_processed_path, 'r', encoding='utf-8') as f:\n",
    "        processed_data = json.load(f)\n",
    "\n",
    "    for top_level_key in top_level_keys:\n",
    "        count_dict = {'original': 0, 'processed': 0, 'remaining': 0}\n",
    "        if top_level_key in original_data:\n",
    "            unprocessed_data = subtract_dicts(original_data[top_level_key], processed_data.get(top_level_key, {}), count_dict)\n",
    "\n",
    "            results[top_level_key] = {\n",
    "                \"已处理的书籍数量\": count_dict['processed'],\n",
    "                \"未处理的书籍数量\": count_dict['remaining'],\n",
    "                \"总书籍数量\": count_dict['processed']+count_dict['remaining'],\n",
    "                \"处理比例\": count_dict['processed'] / (count_dict['processed']+count_dict['remaining']) if (count_dict['processed']+count_dict['remaining']) else 0,\n",
    "            }\n",
    "\n",
    "            export_path = os.path.join(export_json_folder, f\"{top_level_key}_unprocessed_books.json\")\n",
    "            with open(export_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump({top_level_key: unprocessed_data}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'工商管理类': {'已处理的书籍数量': 69,\n",
       "  '未处理的书籍数量': 9,\n",
       "  '总书籍数量': 78,\n",
       "  '处理比例': 0.8846153846153846}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_json_path=step_1_remain_path\n",
    "data=js.read_json(original_json_path)\n",
    "data_list=[]\n",
    "for k,v in data.items():\n",
    "    data_list.append(k)\n",
    "find_unprocessed_books(original_json_path,os.path.join(step_1_processed_path,'step_1_processed.json'),export_json_folder=step_1_unprocessed_path,top_level_keys=data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_and_export_excel(catalog_file_path, step_1_processed_path, step_1_unprocessed_path, step_1_remain_path):\n",
    "    # 读取分类数据\n",
    "    data = js.read_json(catalog_file_path)\n",
    "    new_data_list = [key for key in data.keys()]\n",
    "    lv1_key_names = new_data_list\n",
    "    \n",
    "    # 找到未处理的书籍\n",
    "    unprocessed_data = find_unprocessed_books(catalog_file_path, step_1_processed_path, step_1_unprocessed_path, lv1_key_names)\n",
    "    \n",
    "    # 读取未处理的书籍数据\n",
    "    directory = step_1_unprocessed_path\n",
    "    all_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                for key, value in data.items():\n",
    "                    if key in all_data:\n",
    "                        all_data[key].extend(value)\n",
    "                    else:\n",
    "                        all_data[key] = value\n",
    "    \n",
    "    # 将所有数据写入到一个新的JSON文件中\n",
    "    with open(step_1_remain_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(all_data, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # 将数据转换为DataFrame格式\n",
    "    data = {\n",
    "        '类别': [],\n",
    "        '已处理的书籍数量': [],\n",
    "        '未处理的书籍数量': [],\n",
    "        '总书籍数量': [],\n",
    "        '处理比例': []\n",
    "    }\n",
    "\n",
    "    for category, info in unprocessed_data.items():\n",
    "        data['类别'].append(category)\n",
    "        data['已处理的书籍数量'].append(info['已处理的书籍数量'])\n",
    "        data['未处理的书籍数量'].append(info['未处理的书籍数量'])\n",
    "        data['总书籍数量'].append(info['总书籍数量'])\n",
    "        data['处理比例'].append(info['处理比例'])\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 将DataFrame保存为Excel文件\n",
    "    excel_path = '处理情况.xlsx'\n",
    "    df.to_excel(excel_path, index=False)\n",
    "\n",
    "    print(f\"数据已保存到 {excel_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **辅助处理：功能块1_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data_and_export_excel(catalog_file_path, step_1_processed_path, step_1_unprocessed_path, step_1_remain_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识点第二层聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化线程锁和队列\n",
    "lock = threading.Lock()\n",
    "tasks_queue = Queue()  # 待认领任务的队列\n",
    "merged_results={}\n",
    "\n",
    "#单片任务执行\n",
    "def transform_function(input_dict, NotSuccess_count=0) -> dict:\n",
    "    file_name = input_dict['学科名']\n",
    "    lv1, lv2, lv3, _ = file_name.split('_')\n",
    "    pre_prompt = f'这是{file_name}一些教材的知识点：{input_dict}我希望你帮我融合以上知识点列表，对于属于此学科的知识点，相同的部分合并精炼，不同的部分互相补充，并自行聚类总结知识点，对于无关内容直接摒弃，输出的结果最好是如下结构：'\n",
    "    pro_prompt = f'恰当地设置聚类标准，使得一个类别下的内容相似度尽可能高，同时一个类别下的内容数量尽可能少。并且，这些类别确实能够组织起这个学科的知识体系。请注意，只要两层结构'\n",
    "    prompt = js.generate_prompt(level=3, pre_prompt=pre_prompt, pro_prompt=pro_prompt, words='知识点')\n",
    "    NotSuccess = True\n",
    "    while NotSuccess:\n",
    "        try:\n",
    "            response = service.ask_once(prompt)\n",
    "            result = js.parse_dict(response)\n",
    "            if result:\n",
    "                NotSuccess = False\n",
    "                print(type(result), result)\n",
    "                return result, lv1, lv2, lv3\n",
    "            else:\n",
    "                print('无结果')\n",
    "                NotSuccess_count += 1\n",
    "                if NotSuccess_count >= 2:\n",
    "                    print(\"已经连续2次未成功解析，跳过当前处理\")\n",
    "                    return None\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"发生异常: {e}\")\n",
    "            if 'Error code: 400' in str(e):\n",
    "                # 如果异常信息中包含'400'，则是400错误，直接跳过当前处理\n",
    "                print(\"发生400错误，跳过当前处理\")\n",
    "                return None\n",
    "            elif 'Error code: 429' in str(e):\n",
    "                # 如果异常信息中包含'429'，则是429错误，等待30秒后继续执行\n",
    "                print(\"发生429错误，等待30秒后继续执行\")\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "            else:\n",
    "                # 其他类型的异常，可以添加相应的处理代码\n",
    "                return None\n",
    "\n",
    "#切片\n",
    "def segment_dict(input_dict, file_name, window_length=1800):\n",
    "    segments = []  # 用于存储所有的小字典（即数据段）\n",
    "    current_segment = {'学科名': file_name}  # 当前正在构建的小字典\n",
    "    current_length = 0  # 当前小字典的累计长度\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        item = {key: value}  # 当前遍历到的键值对构成的小字典\n",
    "        item_length = len(str(item))  # 估算当前键值对的长度\n",
    "\n",
    "        # 如果单个item的长度就超过window_length，直接单个成段\n",
    "        if item_length > window_length:\n",
    "            if current_segment != {'学科名': file_name}:  # 确保当前段如果有内容就先保存\n",
    "                segments.append(current_segment)\n",
    "                current_segment = {'学科名': file_name}  # 重置当前段\n",
    "                current_length = 0\n",
    "            current_segment.update(item)\n",
    "            segments.append(current_segment)  # 直接将当前item作为新的段添加\n",
    "            continue\n",
    "\n",
    "        # 判断是否将当前键值对添加到当前小字典，或者开始新的小字典\n",
    "        if current_length + item_length <= window_length:\n",
    "            current_segment.update(item)\n",
    "            current_length += item_length\n",
    "        else:\n",
    "            segments.append(current_segment)  # 保存当前小字典\n",
    "            current_segment = {'学科名': file_name}  # 重置当前段\n",
    "            current_segment.update(item)  # 开始新的小字典\n",
    "            current_length = item_length\n",
    "\n",
    "    # 确保最后一个小字典也被添加\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "\n",
    "    return segments\n",
    "\n",
    "#输出格式处理函数\n",
    "def convert_sets_to_lists(data):\n",
    "    \"\"\"\n",
    "    递归遍历数据结构，将所有的set转换为list。\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            data[key] = convert_sets_to_lists(value)\n",
    "    elif isinstance(data, list):\n",
    "        for i, item in enumerate(data):\n",
    "            data[i] = convert_sets_to_lists(item)\n",
    "    elif isinstance(data, set):\n",
    "        return list(data)\n",
    "    return data\n",
    "\n",
    "def process_task(task):\n",
    "    # 线程安全地处理任务并更新结果\n",
    "    result, lv1, lv2, lv3 = transform_function(task)\n",
    "    \n",
    "    with lock:  # 确保合并操作的线程安全\n",
    "        # 确保嵌套层次存在\n",
    "        if lv1 not in merged_results:\n",
    "            merged_results[lv1] = {}\n",
    "        if lv2 not in merged_results[lv1]:\n",
    "            merged_results[lv1][lv2] = {}\n",
    "        if lv3 not in merged_results[lv1][lv2]:\n",
    "            merged_results[lv1][lv2][lv3] = {}\n",
    "\n",
    "        # 将结果合并到全局字典中\n",
    "        for key, value in result.items():\n",
    "            if key not in merged_results[lv1][lv2][lv3]:\n",
    "                merged_results[lv1][lv2][lv3][key] = value\n",
    "            else:\n",
    "                merged_results[lv1][lv2][lv3][key].extend(value)  # 假设值是列表\n",
    "\n",
    "def process_file_task(file_name, input_directory):\n",
    "    \"\"\"处理单个文件，将其分段后的任务添加到任务队列\"\"\"\n",
    "    full_file_path = os.path.join(input_directory, file_name)\n",
    "    print(f\"正在处理文件: {full_file_path}\")\n",
    "\n",
    "    with open(full_file_path, 'r', encoding='utf-8') as file:\n",
    "        input_dict = json.load(file)\n",
    "        # 对文件内容进行分段\n",
    "        segments = segment_dict(input_dict, file_name)\n",
    "        # 将分段后的任务添加到队列\n",
    "        for segment in segments:\n",
    "            tasks_queue.put(segment)\n",
    "\n",
    "def process_all_json_files(input_directory, output_directory, num_threads=4):\n",
    "    # 读取输入目录下的所有文件并初始化任务队列\n",
    "    file_names = [file for file in os.listdir(input_directory) if file.endswith('_results.json')]\n",
    "    for file_name in file_names:\n",
    "        process_file_task(file_name, input_directory)\n",
    "\n",
    "    # 使用线程池处理分段后的所有任务\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # 提交分段处理任务到线程池\n",
    "        while not tasks_queue.empty():\n",
    "            task = tasks_queue.get_nowait()  # 使用get_nowait避免阻塞\n",
    "            executor.submit(process_task, task)\n",
    "\n",
    "    # 等待所有分段处理任务完成\n",
    "    executor.shutdown(wait=True)\n",
    "\n",
    "    # 将合并后的结果转换为所需格式\n",
    "    final_result = convert_sets_to_lists(merged_results)\n",
    "\n",
    "    # 存储处理结果到指定位置\n",
    "    output_file_path = os.path.join(output_directory, 'transformed_results.json')\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(final_result, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **主处理功能块2：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定输入目录和输出目录，以及线程数\n",
    "\n",
    "input_directory = step_1_processed_path\n",
    "output_directory = step_2_processed_path\n",
    "num_threads = 50\n",
    "\n",
    "process_all_json_files(input_directory, output_directory, num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 期待对数据结构做如下处理：\n",
    "- 统一最末端为{key:null}\n",
    "- 建立key_path_mapping\n",
    "- 添加embedding_mapping并寄存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_processed_json_path=os.path.join(step_2_processed_path,'transformed_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 格式统一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_list_items_to_dict(data):\n",
    "    \"\"\"\n",
    "    递归遍历字典结构，将所有包含列表项的字典转换为 {sub_key: null, ...} 的形式。\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                # 递归处理字典结构\n",
    "                convert_list_items_to_dict(value)\n",
    "            elif isinstance(value, list):\n",
    "                # 处理列表项\n",
    "                new_dict = {}\n",
    "                for item in value:\n",
    "                    new_dict[item] = None\n",
    "                data[key] = new_dict\n",
    "    return data\n",
    "\n",
    "def process_json_file(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    读取指定源文件地址的 JSON 文件，将其中所有包含列表项的字典转换为 {sub_key: null, ...} 的形式，\n",
    "    并将处理后的数据保存到指定输出地址的新的 JSON 文件中。\n",
    "    \"\"\"\n",
    "    # 读取 JSON 文件\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # 转换数据\n",
    "    converted_data = convert_list_items_to_dict(data)\n",
    "    \n",
    "    # 写入新的 JSON 文件\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(converted_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **执行块3_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_json_file(step_2_processed_json_path,step_2_processed_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立键和路径映射\n",
    "mappings=\n",
    "[  \n",
    "{  \n",
    "  'key_name': key,  \n",
    "  'key_path': new_prefix.strip('_'),  \n",
    "  'depth': depth  \n",
    "},  \n",
    "...  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_key_path_mapping(data, prefix='', depth=0, folder_path=''):\n",
    "    \"\"\"\n",
    "    递归遍历字典结构，建立每个键的名称、路径和深度的映射关系，并将结果写入到文件中，\n",
    "    排除最深层的字典。\n",
    "    \"\"\"\n",
    "    mapping_dict = {}\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # 检查当前字典是否为最深层的字典\n",
    "        if all(not isinstance(value, dict) for value in data.values()):\n",
    "            return mapping_dict\n",
    "\n",
    "        for key, value in data.items():\n",
    "            # 构建当前键的完整路径\n",
    "            new_prefix = f\"{prefix}_{key}\" if prefix else key\n",
    "            # 记录当前键的信息\n",
    "            key_info = {\n",
    "                'key_name': key,\n",
    "                'key_path': new_prefix.strip('_'),\n",
    "                'depth': depth\n",
    "            }\n",
    "            mapping_dict[new_prefix.strip('_')] = key_info\n",
    "\n",
    "            # 递归处理子字典或列表中的字典\n",
    "            if isinstance(value, dict):\n",
    "                mapping_dict.update(build_key_path_mapping(value, prefix=new_prefix, depth=depth + 1, folder_path=folder_path))\n",
    "            elif isinstance(value, list):\n",
    "                for i, sub_dict in enumerate(value):\n",
    "                    if isinstance(sub_dict, dict):\n",
    "                        mapping_dict.update(build_key_path_mapping(sub_dict, prefix=f\"{new_prefix}_{i}\", depth=depth + 1, folder_path=folder_path))\n",
    "\n",
    "        # 写入文件\n",
    "        if folder_path and depth == 0:  # 仅在最外层函数调用时写入文件\n",
    "            file_path = os.path.join(folder_path, 'mapping_dict.json')\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(mapping_dict, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **执行块3_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=js.read_json(step_2_processed_json_path)\n",
    "folder_path=step_2_mapping_path\n",
    "mapping_dict=build_key_path_mapping(data,folder_path=folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立包含embedding的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def embed_and_export_dict_batch(given_dict, model_path, json_file_path, batch_size=32, stats_interval=5):\n",
    "    # 加载模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    model.eval()  # 确保模型处于评估模式\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 准备数据，以(key, item)的形式迭代\n",
    "    items = list(given_dict.items())\n",
    "    total_keys = len(items)\n",
    "    batches_processed = 0\n",
    "\n",
    "    for i in range(0, total_keys, batch_size):\n",
    "        batch_items = items[i:i+batch_size]\n",
    "        batch_key_names = [item[1][\"key_name\"] for item in batch_items]\n",
    "        inputs = tokenizer(batch_key_names, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "        # 将嵌入向量添加回原字典\n",
    "        for j, (key, _) in enumerate(batch_items):\n",
    "            given_dict[key][\"embedding\"] = embeddings[j].tolist()\n",
    "\n",
    "        batches_processed += 1\n",
    "\n",
    "        # 统计并打印当前速度\n",
    "        if batches_processed % stats_interval == 0 or (i + batch_size) >= total_keys:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            keys_processed = min((batches_processed) * batch_size, total_keys)\n",
    "            print(f\"已处理 {keys_processed}/{total_keys} 个key_name，耗时 {elapsed_time:.2f}秒，速度：{keys_processed / elapsed_time:.2f}个key_name/秒\")\n",
    "\n",
    "    # 全部处理完毕后一次性保存到JSON文件\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(given_dict, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"完成，已将更新后的字典导出到指定的JSON文件中。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **执行块3_3**\n",
    "在这里我们得到包含着embedding的键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_dict=mapping_dict\n",
    "json_file_path=os.path.join(step_2_mapping_path,'mapping_embedding_dict.json')\n",
    "embed_and_export_dict_batch(given_dict,model_path,json_file_path,batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐层比对融合\n",
    "还回末端字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_null_values_back(original_dict, given_dict, current_path=[], export_folder=None):\n",
    "    \"\"\"\n",
    "    递归地向给定字典中添加原本为null的键值对。\n",
    "    \n",
    "    :param original_dict: 原始的字典。\n",
    "    :param given_dict: 经过处理的字典，需要在这个字典中添加值。\n",
    "    :param current_path: 当前的遍历路径，用于构建给定字典中的键。\n",
    "    :param export_folder: 导出文件夹的路径。\n",
    "    \"\"\"\n",
    "    for key, value in original_dict.items():\n",
    "        # 更新当前路径\n",
    "        new_path = current_path + [key]\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            # 检查是否是最底层的dict\n",
    "            if all(v is None for v in value.values()):\n",
    "                # 如果是最底层的dict，构建在given_dict中对应的键\n",
    "                given_dict_key = \"_\".join(new_path)\n",
    "                # 在given_dict中找到对应的项并添加\"value\"键\n",
    "                given_dict_item = given_dict.get(given_dict_key)\n",
    "                if given_dict_item is not None:\n",
    "                    given_dict_item['value'] = {sub_key: None for sub_key in value}\n",
    "            else:\n",
    "                # 如果不是最底层的dict，继续递归遍历\n",
    "                add_null_values_back(value, given_dict, new_path, export_folder=None)\n",
    "                \n",
    "    # 导出结果到文件\n",
    "    if export_folder:\n",
    "        export_file_path = os.path.join(export_folder, \"mapping_with_embedding_value.json\")\n",
    "        with open(export_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(given_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **执行块3_4** 存入指定文件'mapping_embedding_dict.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_2_processed_json_path=os.path.join(step_2_processed_path,'transformed_results.json')\n",
    "json_file_path=os.path.join(step_2_mapping_path,'mapping_embedding_dict.json')\n",
    "original_dict=js.read_json(step_2_processed_json_path)\n",
    "given_dict=js.read_json(json_file_path)\n",
    "export_folder=step_2_mapping_path\n",
    "# 示例使用\n",
    "add_null_values_back(original_dict, given_dict,export_folder=export_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逐层深入，迭代构建并聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_matrix(vectors):\n",
    "    \"\"\"构建余弦相似度矩阵\"\"\"\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # 避免除以零\n",
    "    normalized_vectors = vectors / norms\n",
    "    similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "def build_mapping_relation(data, depth, similarity_threshold=0.8):\n",
    "    groups = {}\n",
    "    # 分组\n",
    "    for key, item in data.items():\n",
    "        if item['depth'] == depth:  # 只处理depth=3的键\n",
    "            prefix = \"_\".join(key.split(\"_\")[:3])\n",
    "            groups.setdefault(prefix, []).append(key)\n",
    "\n",
    "    # 初始化映射关系为字典\n",
    "    mapping_relation = {}\n",
    "\n",
    "    for group, keys in groups.items():\n",
    "        if len(keys) < 2:  # 单个键无需比较\n",
    "            continue\n",
    "        # 构建矩阵\n",
    "        vectors = np.array([data[key]['embedding'] for key in keys])\n",
    "        # 计算余弦相似度矩阵\n",
    "        sim_matrix = cosine_similarity_matrix(vectors)\n",
    "        \n",
    "        for i, key in enumerate(keys):\n",
    "            # 找到与当前键相似度超过阈值的所有键\n",
    "            sim_indices = np.where(sim_matrix[i] > similarity_threshold)[0]\n",
    "            sim_indices = sim_indices[sim_indices != i]  # 排除自己\n",
    "            if len(sim_indices) == 0:\n",
    "                continue  # 没有超过阈值的相似键，跳过\n",
    "\n",
    "            # 获取相似度超过阈值的键列表\n",
    "            similar_keys = [keys[j] for j in sim_indices]\n",
    "            # 检查当前键是否已经作为其他键的相似项被并入\n",
    "            if not any(key in v for v in mapping_relation.values()):\n",
    "                mapping_relation[key] = similar_keys\n",
    "\n",
    "    return mapping_relation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_embeddings(data,depth):\n",
    "    \"\"\"\n",
    "    移除depth小于3的键的embedding属性。\n",
    "    \"\"\"\n",
    "    for key, item in data.items():\n",
    "        if item['depth'] < depth and 'embedding' in item:\n",
    "            del item['embedding']\n",
    "\n",
    "def merge_dicts(data, mapping_relation):\n",
    "    \"\"\"\n",
    "    根据并射集合并字典项。\n",
    "    \"\"\"\n",
    "    for target_key, source_keys in mapping_relation.items():\n",
    "        for source_key in source_keys:\n",
    "            source_item = data.get(source_key)\n",
    "            target_item = data.get(target_key)\n",
    "            if source_item and target_item:\n",
    "                if 'value' in target_item:\n",
    "                    target_item['value'].update(source_item.get('value', {}))\n",
    "                else:\n",
    "                    target_item['value'] = source_item.get('value', {})\n",
    "\n",
    "def update_keys(data, mapping_relation):\n",
    "    \"\"\"\n",
    "    更新字典中的键路径。\n",
    "    \"\"\"\n",
    "    # 反向构建路径替换映射\n",
    "    replacement_map = {source_key: target_key for target_key, source_keys in mapping_relation.items() for source_key in source_keys}\n",
    "    # 更新数据\n",
    "    new_data = {}\n",
    "    for key, item in data.items():\n",
    "        # 检查当前键是否需要被替换\n",
    "        new_key = replacement_map.get(key, key)\n",
    "        new_data[new_key] = item\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_data(data, start_depth=3, similarity_threshold=0.8):\n",
    "    max_depth = max(item['depth'] for item in data.values())  # 获取最大深度\n",
    "    for depth in range(start_depth, max_depth + 1):\n",
    "        # 步骤1: 构建映射关系\n",
    "        mapping_relation = build_mapping_relation(data, depth, similarity_threshold)\n",
    "        \n",
    "        # 步骤2: 移除embedding属性\n",
    "        remove_embeddings(data, depth)\n",
    "        \n",
    "        # 步骤3: 合并字典项\n",
    "        merge_dicts(data, mapping_relation)\n",
    "        \n",
    "        # 步骤4: 更新键路径\n",
    "        data = update_keys(data, mapping_relation)\n",
    "    for k,v in data.items():\n",
    "        if 'embedding' in v:\n",
    "            del v['embedding']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **执行块3_5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=js.read_json(os.path.join(step_2_mapping_path,'mapping_with_embedding_value.json'))\n",
    "new_data=process_data(data)\n",
    "output_path=os.path.join(step_2_mapping_path,'aggregated_dict.json')\n",
    "js.write_json(new_data,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=os.path.join(step_2_mapping_path,'aggregated_dict.json')\n",
    "data=js.read_json(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重建树形结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_tree_structure(flat_dict):\n",
    "    root = {}\n",
    "\n",
    "    # 遍历每个键值对构建树\n",
    "    for full_path, item in flat_dict.items():\n",
    "        # 分割路径\n",
    "        parts = full_path.split(\"_\")\n",
    "        current_level = root\n",
    "\n",
    "        # 遍历路径的每一部分，逐层深入\n",
    "        for part in parts[:-1]:\n",
    "            # 如果当前层级还没有这个部分的键，则创建一个新的字典\n",
    "            if part not in current_level:\n",
    "                current_level[part] = {}\n",
    "            current_level = current_level[part]\n",
    "\n",
    "        # 对于value字段，需要特别处理\n",
    "        if 'value' in item and item['value']:\n",
    "            # 如果当前节点下有value，则将其作为当前节点的子节点\n",
    "            current_level[parts[-1]] = {k: None for k in item['value']}\n",
    "        else:\n",
    "            # 如果没有value字段，或者value为空，则直接将该节点置为null\n",
    "            current_level[parts[-1]] = None\n",
    "\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_tree(dictionary):\n",
    "    tree = {}\n",
    "    for key, value in dictionary.items():\n",
    "        keys = key.split('_')\n",
    "        current_level = tree\n",
    "        for k in keys:\n",
    "            if k not in current_level:\n",
    "                current_level[k] = {}\n",
    "            current_level = current_level[k]\n",
    "        # 确保value是一个字典且包含'value'键，然后更新current_level\n",
    "        if isinstance(value, dict) and \"value\" in value:\n",
    "            # 确保current_level是一个字典\n",
    "            if isinstance(current_level, dict):\n",
    "                current_level.update(value[\"value\"])\n",
    "            else:\n",
    "                # 如果current_level不是字典，这可能是逻辑上的错误\n",
    "                print(f\"Unexpected type for current_level: {type(current_level)}\",k)\n",
    "    \n",
    "    # 遍历树，将空字典替换为None\n",
    "    def replace_empty_with_none(node):\n",
    "        for k, v in node.items():\n",
    "            if isinstance(v, dict) and not v:  # 空字典 {}\n",
    "                node[k] = None\n",
    "            elif isinstance(v, dict):\n",
    "                replace_empty_with_none(v)\n",
    "    \n",
    "    replace_empty_with_none(tree)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = rebuild_tree(data)\n",
    "js.write_json(tree,'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dict_to_hierarchy(dictionary, depth=0):\n",
    "    result = ''\n",
    "    for key, value in dictionary.items():\n",
    "        result += '  ' * depth + '- ' + key + '\\n'\n",
    "        if isinstance(value, dict):\n",
    "            result += format_dict_to_hierarchy(value, depth + 1)\n",
    "    return result\n",
    "\n",
    "def write_hierarchy_to_file(formatted_hierarchy, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(formatted_hierarchy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
