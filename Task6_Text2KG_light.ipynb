{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from local_packages import *\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import concurrent.futures\n",
    "import random\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "model_path = r'D:\\Joining\\Models\\Text2Vec_base_zh'#填embedding模型的地址\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "agentopener=AgentOpener(service_type='qwen')\n",
    "service=agentopener.service\n",
    "js=JSProcessor()\n",
    "embedder=Embedder(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置工作目录\n",
    "期待输出的json能够在这个目录下遵循以下结构：\n",
    "- /root_folder\n",
    "    - raw_data.txt\n",
    "    - raw_data.json\n",
    "    - /step_1_process\n",
    "        - /step_1_processed_entities\n",
    "            - entity_para_index.json\n",
    "            - entity_partitions.json\n",
    "            - entity_recog_01.json\n",
    "            - step_1_processed.json\n",
    "        - /step_1_unprocessed\n",
    "\n",
    "    - /step_2_process\n",
    "        - /graph_structure\n",
    "            - entities.json\n",
    "            - relations.json\n",
    "            - nodes.json\n",
    "            - edges.json\n",
    "        - /graph_picture\n",
    "            - html\n",
    "            - style.css\n",
    "\n",
    "- model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 定义根文件夹和模型路径\n",
    "root_folder = 'Task6_Text2KG_light'\n",
    "\n",
    "# 设置各个路径\n",
    "raw_data_txt_path = os.path.join(root_folder, \"raw_data.txt\")\n",
    "raw_data_json_path = os.path.join(root_folder, \"raw_data.json\")\n",
    "\n",
    "step_1_process_folder = os.path.join(root_folder, \"step_1_process\")\n",
    "step_1_processed_entities_folder = os.path.join(step_1_process_folder, \"step_1_processed_entities\")\n",
    "entity_para_index_path = os.path.join(step_1_processed_entities_folder, \"entity_para_index.json\")\n",
    "entity_partitions_path = os.path.join(step_1_processed_entities_folder, \"entity_partitions.json\")\n",
    "entity_recog_01_path = os.path.join(step_1_processed_entities_folder, \"entity_recog_01.json\")\n",
    "step_1_processed_json_path = os.path.join(step_1_processed_entities_folder, \"step_1_processed.json\")\n",
    "step_1_unprocessed_folder = os.path.join(step_1_process_folder, \"step_1_unprocessed\")\n",
    "\n",
    "step_2_process_folder = os.path.join(root_folder, \"step_2_process\")\n",
    "graph_structure_folder = os.path.join(step_2_process_folder, \"graph_structure\")\n",
    "entities_json_path = os.path.join(graph_structure_folder, \"entities.json\")\n",
    "relations_json_path = os.path.join(graph_structure_folder, \"relations.json\")\n",
    "nodes_json_path = os.path.join(graph_structure_folder, \"nodes.json\")\n",
    "edges_json_path = os.path.join(graph_structure_folder, \"edges.json\")\n",
    "\n",
    "graph_picture_folder = os.path.join(step_2_process_folder, \"graph_picture\")\n",
    "\n",
    "html_path = os.path.join(graph_picture_folder, \"nodes_and_edges.html\")\n",
    "\n",
    "# 创建所有需要的目录\n",
    "directories = [\n",
    "    root_folder,\n",
    "    step_1_process_folder,\n",
    "    step_1_processed_entities_folder,\n",
    "    step_1_unprocessed_folder,\n",
    "    step_2_process_folder,\n",
    "    graph_structure_folder,\n",
    "    graph_picture_folder\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# 创建所有需要的文件\n",
    "files = [\n",
    "    raw_data_txt_path,\n",
    "    raw_data_json_path,\n",
    "    entity_para_index_path,\n",
    "    entity_partitions_path,\n",
    "    entity_recog_01_path,\n",
    "    step_1_processed_json_path,\n",
    "    entities_json_path,\n",
    "    relations_json_path,\n",
    "    nodes_json_path,\n",
    "    edges_json_path,\n",
    "    html_path\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    if not os.path.exists(file):\n",
    "        with open(file, 'w', encoding='utf-8') as f:\n",
    "            if file.endswith('.json'):\n",
    "                f.write('{}')  # 写入空的JSON对象\n",
    "            elif file.endswith('.txt'):\n",
    "                f.write('')  # 创建空的txt文件\n",
    "            elif file.endswith('.html'):\n",
    "                f.write('<html></html>')  # 创建简单的HTML文件\n",
    "\n",
    "print(\"所有目录和文件已成功创建。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "import queue\n",
    "from queue import Queue, Empty\n",
    "from threading import Thread, Lock\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ParseError(Exception):\n",
    "    def __init__(self, code, message):\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "        super().__init__(self.message)\n",
    "\n",
    "def step0_str_to_json(input_str):\n",
    "    # 将字符串按双换行符分割成大段\n",
    "    paragraphs = input_str.strip().split('\\n\\n')\n",
    "    \n",
    "    # 创建一个列表来存储合并后的段落\n",
    "    merged_paragraphs = []\n",
    "    temp_paragraph = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        sub_paragraphs = paragraph.split('\\n')\n",
    "        for sub_paragraph in sub_paragraphs:\n",
    "            if len(temp_paragraph) == 0:\n",
    "                temp_paragraph = sub_paragraph.strip()\n",
    "            else:\n",
    "                if len(temp_paragraph) + len(sub_paragraph.strip()) < 200:\n",
    "                    temp_paragraph += \" \" + sub_paragraph.strip()\n",
    "                else:\n",
    "                    merged_paragraphs.append(temp_paragraph)\n",
    "                    temp_paragraph = sub_paragraph.strip()\n",
    "\n",
    "        # 检查temp_paragraph的长度，如果大于200字则进行分割\n",
    "        while len(temp_paragraph) > 400:\n",
    "            split_index = temp_paragraph.rfind(' ', 0, 400)\n",
    "            if split_index == -1:\n",
    "                split_index = 400\n",
    "            merged_paragraphs.append(temp_paragraph[:split_index])\n",
    "            temp_paragraph = temp_paragraph[split_index:].strip()\n",
    "\n",
    "    # 将最后一个段落加入列表\n",
    "    if temp_paragraph:\n",
    "        merged_paragraphs.append(temp_paragraph)\n",
    "    \n",
    "    # 创建一个字典，将每个段落存储在字典中\n",
    "    result = {}\n",
    "    for i, paragraph in enumerate(merged_paragraphs, 1):\n",
    "        result[f'paragraph_{i}'] = {'text': paragraph}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def step0_read_txt_to_str(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def step1_parse_single_file(original_single_dict: dict, target_keys: list) -> dict:\n",
    "    result = {}\n",
    "    text = str(original_single_dict.get('text', '未提供'))\n",
    "    key = target_keys[0]\n",
    "    sys_prompt = f'''\n",
    "    你是一个文本分析师，对于用户输入的文本，我要求你输出一个python列表，其中的值都是知识点、实体或概念，以如下结构的一个python列表返回，由中括号所表示：\n",
    "    ['entity1','entity2','entity3',...]\n",
    "    注意：务必精炼，省略一切无关内容\n",
    "    '''\n",
    "    user_prompt = f'''\n",
    "    用户输入的文本为{text}\n",
    "    '''\n",
    "    try:\n",
    "        msg = service.ask_once(sys_prompt,user_prompt)\n",
    "        parse_success = js.parse_list(msg)\n",
    "        if parse_success:\n",
    "            result[key] = parse_success\n",
    "            print('1 success')\n",
    "        else:\n",
    "            raise ParseError(1001, f\"解析失败：书名 {key}\")\n",
    "    except Exception as e:\n",
    "        error_message = str(e)\n",
    "        if 'Error code: 400' in error_message:\n",
    "            raise ParseError(400, \"发生400错误，跳过当前处理\")\n",
    "        elif 'Error code: 429' in error_message:\n",
    "            raise ParseError(429, \"发生429错误，等待30秒后继续执行\")\n",
    "        else:\n",
    "            raise ParseError(1000, f\"发生未知错误{e}\")\n",
    "    return result\n",
    "\n",
    "def step1_worker(task_key, original_single_dict, lock, result_dict):\n",
    "    target_keys = [task_key]\n",
    "    retry_count = 0\n",
    "    while retry_count <= 2:\n",
    "        try:\n",
    "            parsed_result = step1_parse_single_file(original_single_dict, target_keys)\n",
    "            with lock:\n",
    "                result_dict[task_key] = parsed_result\n",
    "            break\n",
    "        except ParseError as e:\n",
    "            if e.code == 1001:\n",
    "                retry_count += 1\n",
    "            elif e.code == 429 or 'Throttling.RateQuota' in str(e):\n",
    "                print(f\"429 或 Throttling.RateQuota 错误: {task_key}，将在20秒后重试... (尝试 {retry_count + 1})\")\n",
    "                time.sleep(20)\n",
    "                retry_count += 1\n",
    "            else:\n",
    "                break\n",
    "# 抽取原始概念的多线程\n",
    "def step1_multi_thread_parse(original_dict: dict, thread_number: int) -> dict:\n",
    "    result_dict = {}\n",
    "    lock = threading.Lock()\n",
    "    tasks = [(task_key, value_dict) for task_key, value_dict in original_dict.items()]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=thread_number) as executor:\n",
    "        future_to_task = {executor.submit(step1_worker, task_key, value_dict, lock, result_dict): (task_key, value_dict) for task_key, value_dict in tasks}\n",
    "        for future in as_completed(future_to_task):\n",
    "            task_key, _ = future_to_task[future]\n",
    "            try:\n",
    "                future.result(timeout=30)\n",
    "            except Exception as e:\n",
    "                print(f\"任务 {task_key} 发生错误: {traceback.format_exc()}\")\n",
    "\n",
    "    # 线程安全的写入JSON文件\n",
    "    with lock:\n",
    "        with open(step_1_processed_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return result_dict\n",
    "# 函数：按段落编号排序字典\n",
    "def sort_by_para_index(input_dict):\n",
    "    # 提取键值对并按段落编号排序\n",
    "    sorted_items = sorted(input_dict.items(), key=lambda x: int(x[0].split('_')[-1]))\n",
    "    # 将排序后的键值对转换回字典\n",
    "    sorted_dict = {item[0]: item[1] for item in sorted_items}\n",
    "    return sorted_dict\n",
    "# 函数：合并所有sub_dict中的元素到一个列表\n",
    "def merge_elements_to_single_list(input_dict):\n",
    "    # 创建一个空列表用于存放所有元素\n",
    "    final_list = []\n",
    "    # 遍历原始字典中的每个sub_dict\n",
    "    for sub_dict in input_dict.values():\n",
    "        # 假设每个sub_dict的值是一个列表\n",
    "        elements = sub_dict.values()\n",
    "        # 遍历每个sub_dict中的所有列表，并将元素添加到最终列表中\n",
    "        for element_list in elements:\n",
    "            # 将当前列表的元素添加到最终列表中\n",
    "            final_list.extend(element_list)\n",
    "    # 返回包含所有元素的最终列表\n",
    "    return final_list\n",
    "# 遍历similar_keys_dict的每个key，查找merged_dict中的相关段落\n",
    "def find_related_paragraphs(merged_dict, similar_keys_dict):\n",
    "    result_dict = {}\n",
    "    \n",
    "    for main_key, similar_keys_info in similar_keys_dict.items():\n",
    "        related_paragraphs = {}\n",
    "        \n",
    "        for para_key, para_info in merged_dict.items():\n",
    "            para_entities = para_info[\"entities\"]\n",
    "            para_text = para_info[\"text\"]\n",
    "            \n",
    "            if any(key in para_entities for key in [main_key] + similar_keys_info[\"Similar_keys\"]):\n",
    "                related_paragraphs[para_key] = para_text\n",
    "        \n",
    "        result_dict[main_key] = {\n",
    "            \"related_paragraphs\": related_paragraphs\n",
    "        }\n",
    "    \n",
    "    return result_dict\n",
    "# 生成实体解释的多线程\n",
    "def process_related_paragraphs(num_threads, related_paragraphs_dict):\n",
    "    # 创建一个线程安全的队列\n",
    "    task_queue = queue.Queue()\n",
    "\n",
    "    # 创建一个字典，用于存储结果\n",
    "    result_dict = {}\n",
    "    result_dict_lock = threading.Lock()  # 锁用于确保线程安全\n",
    "\n",
    "    def parse_dict_with_timeout(k, v, timeout=20, max_retries=3):\n",
    "        related_content = ''.join(v['related_paragraphs'].values())\n",
    "        sys_prompt = f'''\n",
    "        你是一个基于原文的概念分析师，不会私自解读，你总是只返回提供给你的那一个概念的定义\n",
    "        '''\n",
    "        user_prompt = f'''\n",
    "        请为我输出：{k}这个概念的定义，基于{related_content}忠实地返回,不许私自解读,请返回一个json字典，以:\n",
    "        {{'概念':'...','定义':'...'}}的格式返回\n",
    "        '''\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return_dict = service.ask_once(sys_prompt,user_prompt)\n",
    "                if 'Throttling.RateQuota' in return_dict:\n",
    "                    raise Exception('Throttling.RateQuota encountered')\n",
    "                parse_success = js.parse_dict(return_dict)\n",
    "                if parse_success:\n",
    "                    with result_dict_lock:\n",
    "                        result_dict[k] = parse_success\n",
    "                        print('success')\n",
    "                    return True\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) or 'Throttling.RateQuota' in str(e):\n",
    "                    print(f\"429 或 Throttling.RateQuota 错误: {k}，将在{40*(attempt+1)}秒后重试... (尝试 {attempt + 1})\")\n",
    "                    time.sleep(40*(attempt+1))\n",
    "                else:\n",
    "                    print(f\"处理 {k} 时出错: {e}, {return_dict}, 尝试 {attempt + 1}\")\n",
    "        return False\n",
    "\n",
    "    def worker():\n",
    "        while True:\n",
    "            item = task_queue.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            k, v = item\n",
    "            try:\n",
    "                with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                    future = executor.submit(parse_dict_with_timeout, k, v)\n",
    "                    success = future.result(timeout=30)\n",
    "                    if not success:\n",
    "                        print(f\"Failed to process {k} after some attempts\")\n",
    "            except Exception as e:\n",
    "                print(f\"Timeout or error processing {k}: {e}\")\n",
    "            finally:\n",
    "                task_queue.task_done()\n",
    "\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        t = threading.Thread(target=worker)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for k, v in related_paragraphs_dict.items():\n",
    "        task_queue.put((k, v))\n",
    "\n",
    "    # 阻塞直到所有任务完成\n",
    "    task_queue.join()\n",
    "\n",
    "    # 停止所有工作线程\n",
    "    for i in range(num_threads):\n",
    "        task_queue.put(None)\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # 将 result_dict 格式化为 JSON 并存入文件\n",
    "    entities_json_path = os.path.join(graph_structure_folder, \"entities.json\")\n",
    "    print(result_dict)\n",
    "    js.write_json(result_dict, entities_json_path)\n",
    "\n",
    "    print(\"所有任务已完成，结果已保存。\")\n",
    "# 生成json的格式化\n",
    "def process_entities(entities_dict, index_dict):\n",
    "    result_dict = {}\n",
    "    \n",
    "    # 遍历 entities_dict\n",
    "    for main_entity, data in entities_dict.items():\n",
    "        related_paragraphs = data.get(\"related_paragraphs\", {})\n",
    "        \n",
    "        # 遍历相关段落\n",
    "        for para_key, para_text in related_paragraphs.items():\n",
    "            if para_key in index_dict:\n",
    "                entities = index_dict[para_key][\"entities\"]\n",
    "                \n",
    "                # 遍历段落中的实体\n",
    "                for entity in entities:\n",
    "                    if entity != main_entity:\n",
    "                        entity_pair = frozenset([main_entity, entity])\n",
    "                        \n",
    "                        # 将实体对加入结果字典\n",
    "                        if entity_pair not in result_dict:\n",
    "                            result_dict[entity_pair] = {\n",
    "                                \"paragraphs\": set()\n",
    "                            }\n",
    "                        result_dict[entity_pair][\"paragraphs\"].add((\n",
    "                            para_key,\n",
    "                            index_dict[para_key][\"text\"]\n",
    "                        ))\n",
    "    \n",
    "    # 将 frozenset 转换成列表，并移除重复内容\n",
    "    final_result_dict = {}\n",
    "    for entity_pair, details in result_dict.items():\n",
    "        entity_list = list(entity_pair)\n",
    "        paragraphs = [\n",
    "            {\"paragraph_key\": para_key, \"text\": para_text}\n",
    "            for para_key, para_text in details[\"paragraphs\"]\n",
    "        ]\n",
    "        final_result_dict[tuple(entity_list)] = {\n",
    "            \"paragraphs\": paragraphs\n",
    "        }\n",
    "    for entity_pair, details in final_result_dict.items():\n",
    "        print(f\"Entity Pair: {entity_pair}\")\n",
    "        for paragraph in details[\"paragraphs\"]:\n",
    "            print(f\"  Paragraph Key: {paragraph['paragraph_key']}\")\n",
    "            print(f\"  Text: {paragraph['text']}\")\n",
    "        print()\n",
    "    return final_result_dict\n",
    "# 生成关系的多线程(这里被沉默了)\n",
    "def process_relations_multithreaded(related_paragraphs_dict, num_threads=15):\n",
    "    task_queue = queue.Queue()\n",
    "    result_dict_lock = threading.Lock()\n",
    "    success_count = 0\n",
    "    failure_count = 0\n",
    "    final_result_dict_2 = {}\n",
    "\n",
    "    def parse_relation(k, v, attempt=1, timeout=30):\n",
    "        nonlocal success_count\n",
    "        entity_1, entity_2 = k\n",
    "        related_context = ''.join(para_dict['text'] for para_dict in v['paragraphs'])\n",
    "        sys_prompt = f'''\n",
    "        你是一名忠实的基于原文的概念分析师，对于不是强相关的概念你会坚决地指出他们无关。你将获得一个概念对 (A, B)、他们的原文，以及一组关系类别。请为每对概念确定关系类型，并表示为有向边 (to/from)、无向边 (and) 或无边 (no)。关系类别及其边表示如下，请你从下面选择一个最恰当的：\n",
    "        \"A 依赖于 B\"\n",
    "        \"A 控制 B\"\n",
    "        \"A 影响 B\"\n",
    "        \"A 约束 B\"\n",
    "        \"A 支持 B\"\n",
    "        \"A 授权 B\"\n",
    "        \"A 将信息或资源传递给 B\"\n",
    "        \"A 引导 B\"\n",
    "        \"A 转换为 B\"\n",
    "        \"A 触发 B\"\n",
    "        \"A 继承 B\"\n",
    "        \"A 是 B 的流程步骤\"\n",
    "        \"A 对 B 提供反馈\"\n",
    "        \"A from B\",\n",
    "        \"A 被 B 依赖\",\n",
    "        \"A 被 B 控制\",\n",
    "        \"A 被 B 影响\",\n",
    "        \"A 被 B 约束\",\n",
    "        \"A 被 B 支持\",\n",
    "        \"A 被 B 授权\",\n",
    "        \"A 被 B 传递信息或资源\",\n",
    "        \"A 被 B 引导\",\n",
    "        \"A 被 B 转换\",\n",
    "        \"A 被 B 触发\",\n",
    "        \"A 被 B 继承\",\n",
    "        \"A 是 B 的流程步骤\",\n",
    "        \"A 被 B 提供反馈\"\n",
    "        \n",
    "        \"A 与 B 是对比关系\"\n",
    "        \"A 与 B 是并列关系\"\n",
    "        \"A 与 B 是相似关系\"\n",
    "        \"A 与 B 是互补关系\"\n",
    "        \"A 与 B 是对称关系\"\n",
    "        \"A 与 B 相关\"\n",
    "\n",
    "        无边 (no)：\n",
    "        \"独立的概念\"\n",
    "        \"A 与 B 是独立的分类\"\n",
    "        \"A 与 B 是独立的类型\"\n",
    "        不相关关系\n",
    "        \"A 与 B 是不相关的\"\n",
    "        \n",
    "        以{{'关系类型':'A ... B','关系名称':'...','说明':'...'}}的形式返回，若无关则关系类型写'无关'\n",
    "        '''\n",
    "        user_prompt = f'''\n",
    "        请你从上述关系中选择一个指出A:{entity_1}与B:{entity_2}之间的关系，根据相关内容：{related_context}，以：{{'关系类型':'A ... B','关系名称':'...','说明':'...'}}的字典格式返回；注意，关系类型务必写 A xx B, 若无关则关系类型写'无关'\n",
    "        '''\n",
    "        try:\n",
    "            parse_success=True\n",
    "            if parse_success:\n",
    "                with result_dict_lock:\n",
    "                    final_result_dict_2[tuple(k)] = {\n",
    "                        'entity_pair': (entity_1, entity_2),\n",
    "                        'relation_type': '',\n",
    "                        'relation_name': '',\n",
    "                        'relation_explaination': ''\n",
    "                    }\n",
    "                with result_dict_lock:\n",
    "                    success_count += 1\n",
    "                print(f\"Success: {entity_1} 和 {entity_2} 的关系已成功解析\")\n",
    "                return True\n",
    "        except ParseError as e:\n",
    "            if \"Error code: 429\" in str(e) and attempt <= 3:  # 最多重试一次\n",
    "                print(f\"429 错误，等待40秒后重试 (第 {attempt} 次)\")\n",
    "                time.sleep(40)\n",
    "                return parse_relation(k, v, attempt + 1, timeout)\n",
    "            else:\n",
    "                print(f\"Error processing {k}: {e}, attempt {attempt}\")\n",
    "        except Exception as e:\n",
    "            if (\"Error code: 429\" in str(e) or 'Throttling.RateQuota' in str(e)) and attempt <= 2:  # 最多重试一次\n",
    "                print(f\"429 或 Throttling.RateQuota 错误，等待{40*(attempt+1)}秒后重试 (第 {attempt} 次)\")\n",
    "                time.sleep(40*(attempt+1))\n",
    "                return parse_relation(k, v, attempt + 1, timeout)\n",
    "            else:            \n",
    "                print(f\"Error processing {k}: {e}, attempt {attempt}\")\n",
    "        return False\n",
    "        \n",
    "    def worker():\n",
    "        nonlocal success_count, failure_count\n",
    "        while True:\n",
    "            item = task_queue.get()\n",
    "            if item is None:\n",
    "                break\n",
    "            k, v = item\n",
    "            try:\n",
    "                with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                    future = executor.submit(parse_relation, k, v)\n",
    "                    success = future.result(timeout=30)\n",
    "                    if not success:\n",
    "                        with result_dict_lock:\n",
    "                            failure_count += 1\n",
    "                        print(f\"Failed to process {k} after 2 attempts\")\n",
    "            except Exception as e:\n",
    "                with result_dict_lock:\n",
    "                    failure_count += 1\n",
    "                print(f\"Timeout or error processing {k}: {e}\")\n",
    "            finally:\n",
    "                task_queue.task_done()\n",
    "\n",
    "    def main():\n",
    "        nonlocal success_count, failure_count\n",
    "        threads = []\n",
    "        for i in range(num_threads):\n",
    "            t = threading.Thread(target=worker)\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        # 统计待处理任务数量\n",
    "        total_tasks = len(related_paragraphs_dict)\n",
    "        print(f\"待处理任务总数: {total_tasks}\")\n",
    "\n",
    "        for k, v in related_paragraphs_dict.items():\n",
    "            task_queue.put((k, v))\n",
    "\n",
    "        # 阻塞直到所有任务完成\n",
    "        task_queue.join()\n",
    "\n",
    "        # 停止所有工作线程\n",
    "        for i in range(num_threads):\n",
    "            task_queue.put(None)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        # 打印任务完成情况\n",
    "        print(f\"总成功任务数: {success_count}\")\n",
    "        print(f\"总失败任务数: {failure_count}\")\n",
    "\n",
    "        # 将结果写入JSON文件\n",
    "        graph_dict = {}\n",
    "        for k, v in final_result_dict_2.items():\n",
    "            graph_dict[str(k)] = v\n",
    "\n",
    "        relations_json_path = os.path.join(graph_structure_folder, \"relations.json\")\n",
    "        js.write_json(graph_dict, relations_json_path)\n",
    "\n",
    "        print(f\"结果已保存到: {relations_json_path}\")\n",
    "\n",
    "    # 调用主函数\n",
    "    main()\n",
    "# 主函数\n",
    "def txt_to_kg(file_path,all_relations, thread_number_1=180,thread_number_2=180,thread_number_3=180,threshold = 0.8 ):\n",
    "\n",
    "    file_content = step0_read_txt_to_str(file_path)\n",
    "#    print(file_content)\n",
    "    json_output = step0_str_to_json(file_content)\n",
    "\n",
    "    js.write_json(json_output, raw_data_json_path)\n",
    "#    print(f\"JSON 数据已写入 {raw_data_json_path}\")\n",
    "\n",
    "    step1_multi_thread_parse(json_output,thread_number=thread_number_1)\n",
    "    \n",
    "    original_dict = js.read_json(step_1_processed_json_path)\n",
    "    sorted_dict = sort_by_para_index(original_dict)\n",
    "    js.write_json(sorted_dict,step_1_processed_json_path)\n",
    "    merged_list = merge_elements_to_single_list(original_dict)\n",
    "        \n",
    "    # 简化字典结构\n",
    "    simplified_dict = {k: v[k] for k, v in sorted_dict.items()}\n",
    "\n",
    "    entities_dict=simplified_dict\n",
    "    text_dict=js.read_json(raw_data_json_path)\n",
    "\n",
    "    # 合并字典\n",
    "    merged_dict = {}\n",
    "    for key in text_dict:\n",
    "        merged_dict[key] = {\n",
    "            \"entities\": entities_dict.get(key, []),\n",
    "            \"text\": text_dict[key][\"text\"]\n",
    "        }\n",
    "\n",
    "    # 打印合并后的字典\n",
    "    print(merged_dict)\n",
    "    js.write_json(merged_dict,entity_recog_01_path)\n",
    "    given_list=merged_list\n",
    "\n",
    "    json_file_path=os.path.join(step_1_process_folder,'mapping_embedding_dict.json')\n",
    "    embedded_dict=embedder.embed_list(given_list)\n",
    "    print(embedded_dict)\n",
    "    # 进行分区\n",
    "    partitions = embedder.partition_by_similarity(embedded_dict, threshold)\n",
    "\n",
    "    print(partitions)\n",
    "\n",
    "    js.write_json(partitions,entity_partitions_path)\n",
    "\n",
    "    partition_dict=js.read_json(entity_partitions_path)\n",
    "\n",
    "    index_dict=js.read_json(entity_recog_01_path)\n",
    "\n",
    "    # 生成相关段落的字典\n",
    "    related_paragraphs_dict = find_related_paragraphs(index_dict, partition_dict)\n",
    "    js.write_json(related_paragraphs_dict,entity_para_index_path)\n",
    "    process_related_paragraphs(num_threads=thread_number_2,related_paragraphs_dict=related_paragraphs_dict)\n",
    "    entities_dict=js.read_json(entity_para_index_path)\n",
    "    index_dict=js.read_json(entity_recog_01_path) \n",
    "    final_result_dict=process_entities(entities_dict, index_dict)\n",
    "    process_relations_multithreaded(final_result_dict, num_threads=thread_number_3)\n",
    "\n",
    "    # 读取 JSON 文件\n",
    "    dict1 = js.read_json(entities_json_path)\n",
    "    dict2 = js.read_json(relations_json_path)\n",
    "\n",
    "    # 将 dict1 转换为新的格式\n",
    "    new_dict1 = {}\n",
    "    for key, value in dict1.items():\n",
    "        try:\n",
    "            # 解析字符串为字典\n",
    "            value_dict = value\n",
    "            # 提取概念和定义\n",
    "            node_name = value_dict['概念']\n",
    "            node_def = value_dict['定义']\n",
    "            new_dict1[str(node_name)] = {'name': node_name, 'node_content': node_def}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError for key {key}, value: {value}: {e}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError for key {key}, value: {value}: {e}\")\n",
    "\n",
    "    new_dict2 = {}\n",
    "    relation_id = 1\n",
    "    for key, value in dict2.items():\n",
    "        node1 = value[\"entity_pair\"][0]\n",
    "        node2 = value[\"entity_pair\"][1]\n",
    "        if node1 in new_dict1 and node2 in new_dict1:\n",
    "            relation_name = value['relation_name']\n",
    "            relation_type = ''\n",
    "            for r_type, r_list in all_relations.items():\n",
    "                if relation_name in r_list:\n",
    "                    relation_type = r_type\n",
    "                    break\n",
    "            \n",
    "            new_dict2[relation_id] = {\n",
    "                'begin_node': new_dict1[node1]['name'],\n",
    "                'end_node': new_dict1[node2]['name'],\n",
    "                'relation_name': relation_name,\n",
    "                'relation_explaination': value['relation_explaination'],\n",
    "                'relation_type': relation_type\n",
    "            }\n",
    "            relation_id += 1\n",
    "\n",
    "    # 写入新的 JSON 文件\n",
    "    js.write_json(new_dict1, nodes_json_path)\n",
    "    js.write_json(new_dict2, edges_json_path)\n",
    "\n",
    "    import networkx as nx\n",
    "\n",
    "    html_template_path = os.path.join(graph_picture_folder, \"template.html\")\n",
    "    output_html_path = os.path.join(graph_picture_folder, \"nodes_and_edges.html\")\n",
    "\n",
    "    # 读取JSON数据\n",
    "    nodes=js.read_json(nodes_json_path)\n",
    "    edges=js.read_json(edges_json_path)\n",
    "\n",
    "    # 创建一个无向图\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 添加节点\n",
    "    for node_id in nodes.keys():\n",
    "        G.add_node(node_id)\n",
    "\n",
    "    # 添加边\n",
    "    for edge_id, edge_info in edges.items():\n",
    "        G.add_edge(edge_info['begin_node'], edge_info['end_node'], relation_name=edge_info['relation_name'], relation_explaination=edge_info['relation_explaination'])\n",
    "\n",
    "    # 使用spring布局算法来计算节点位置\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # 将位置转换为字典\n",
    "    positions = {node: (float(x), float(y)) for node, (x, y) in pos.items()}\n",
    "\n",
    "    # 将节点和边的数据转换为JSON字符串\n",
    "    nodes_data = json.dumps([{'id': node_id, 'name': node_info['name'], 'content': node_info['node_content']} for node_id, node_info in nodes.items()])\n",
    "    links_data = json.dumps([{'source': edge_info['begin_node'], 'target': edge_info['end_node'], 'name': edge_info['relation_name'], 'explain': edge_info['relation_explaination']} for edge_id, edge_info in edges.items()])\n",
    "\n",
    "    # 读取HTML模板\n",
    "    with open(html_template_path, 'r', encoding='utf-8') as f:\n",
    "        html_template = f.read()\n",
    "\n",
    "    # 替换模板中的占位符\n",
    "    html_content = html_template.replace('{{ nodes }}', nodes_data).replace('{{ links }}', links_data)\n",
    "\n",
    "    # 将生成的HTML内容保存到文件中\n",
    "    with open(output_html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(f\"HTML文件已生成：{output_html_path}\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = {\n",
    "    \"A to B\": [\n",
    "        \"A to B\",\n",
    "        \"A 依赖于 B\",\n",
    "        \"A 控制 B\",\n",
    "        \"A 影响 B\",\n",
    "        \"A 约束 B\",\n",
    "        \"A 支持 B\",\n",
    "        \"A 授权 B\",\n",
    "        \"A 将信息或资源传递给 B\",\n",
    "        \"A 引导 B\",\n",
    "        \"A 转换为 B\",\n",
    "        \"A 触发 B\",\n",
    "        \"A 继承 B\",\n",
    "        \"A 是 B 的流程步骤\",\n",
    "        \"A 对 B 提供反馈\"\n",
    "    ],\n",
    "    \"A from B\": [\n",
    "        \"A 被 B 依赖\",\n",
    "        \"A 被 B 控制\",\n",
    "        \"A 被 B 影响\",\n",
    "        \"A 被 B 约束\",\n",
    "        \"A 被 B 支持\",\n",
    "        \"A 被 B 授权\",\n",
    "        \"A 被 B 传递信息或资源\",\n",
    "        \"A 被 B 引导\",\n",
    "        \"A 被 B 转换\",\n",
    "        \"A 被 B 触发\",\n",
    "        \"A 被 B 继承\",\n",
    "        \"A 是 B 的流程步骤\",\n",
    "        \"A 被 B 提供反馈\"\n",
    "    ],\n",
    "    \"A and B\": [\n",
    "        \"A and B\",\n",
    "        \"A 与 B 是对比关系\",\n",
    "        \"A 与 B 是并列关系\",\n",
    "        \"A 与 B 是相似关系\",\n",
    "        \"A 与 B 是互补关系\",\n",
    "        \"A 与 B 是对称关系\",\n",
    "        \"A 与 B 相关\"\n",
    "    ],\n",
    "    \"A no B\": [\n",
    "        \"无关\",\n",
    "        \"不相关\",\n",
    "        \"A no B\",\n",
    "        \"独立的概念\",\n",
    "        \"A 与 B 是独立的分类\",\n",
    "        \"A 与 B 是独立的类型\",\n",
    "        \"不相关关系\",\n",
    "        \"A 与 B 是不相关的\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "txt_to_kg(raw_data_txt_path,relations, thread_number_1=10, thread_number_2=10, thread_number_3=5, threshold = 0.8 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 (Jiaoy)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
