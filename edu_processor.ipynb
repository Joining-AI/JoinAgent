{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from http.server import HTTPServer, SimpleHTTPRequestHandler\n",
    "from LLM_API import GLMService, SenseService, KimiService\n",
    "from Json_Processor import JSProcessor\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 加载环境变量\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "\n",
    "# 设置项目根目录和图片目录\n",
    "project_root = os.path.dirname(dotenv_path)\n",
    "\n",
    "service_type = 'zhipu'\n",
    "\n",
    "def initialize_service(service_type):\n",
    "    if service_type in ['zhipu', None]:\n",
    "        version = 'glm-3-turbo'\n",
    "        #'glm-4' 'glm-4v' 'glm-3-turbo'\n",
    "        service = GLMService(version)\n",
    "    elif service_type in ['kimi']:\n",
    "        version = '8k'\n",
    "        #'8k'1M/12￥ '32k'1M/24￥ '128k'1M/60￥\n",
    "        service = KimiService(version)\n",
    "    elif service_type in ['sensetime']:\n",
    "        version = 'SenseChat'\n",
    "        #SenseChat SenseChat-32K SenseChat-128K SenseChat-Turbo SenseChat-FunctionCall\n",
    "        service = SenseService(version=version)\n",
    "    else:\n",
    "        raise ValueError('未知的服务类型')\n",
    "    \n",
    "    return service\n",
    "\n",
    "service = initialize_service(service_type)\n",
    "\n",
    "js=JSProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 书本目录处理为知识点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file_path = 'Edu_Resources.json'\n",
    "\n",
    "# 辅助函数：查找特定三级目录\n",
    "def extract_and_parse_json(original_dict, target_keys, service):\n",
    "    # 读取JSON文件\n",
    "\n",
    "    # Extracting content based on target keys\n",
    "    extracted_dict = original_dict\n",
    "    for key in target_keys:\n",
    "        extracted_dict = extracted_dict.get(key, {})\n",
    "    original_list = extracted_dict\n",
    "\n",
    "    result = {}\n",
    "    task_count = 0  # 初始化任务计数器\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "\n",
    "    for dict_item in original_list:\n",
    "        try:\n",
    "            book_name = str(dict_item['书名'])\n",
    "            catalog = str(dict_item['目录'])\n",
    "\n",
    "            prompt = f'''\n",
    "            以下内容是{target_keys[-1]}领域的书籍目录，书名{book_name}，目录内容为：{catalog}，我要求你输出一个列表，其中的值是知识点，必须是如下结构：['知识点1','知识点2','知识点3',...]\n",
    "            省略一切无关内容\n",
    "            '''\n",
    "            NotSuccess = True\n",
    "            while NotSuccess:\n",
    "                msg = service.ask_once(prompt)  # 假设这是从某个服务获取的回应\n",
    "                if js.parse_list(msg):\n",
    "                    NotSuccess = False\n",
    "                    print(f\"成功解析：{book_name}\")\n",
    "                    result[book_name] = js.parse_list(msg)\n",
    "                    task_count += 1  # 成功处理一个任务，计数器加1\n",
    "                    if task_count % 5 == 0:  # 每五个任务\n",
    "                        end_time = time.time()  # 记录结束时间\n",
    "                        print(f\"当前处理速度：{task_count/(end_time - start_time)}个任务/秒，截至目前一共消耗了{service.total_tokens_used*0.000006}元\")\n",
    "                else:\n",
    "                    print(msg)\n",
    "        except Exception as e:\n",
    "            print(f\"发生异常: {e}\")\n",
    "            # 在这里添加处理异常的代码，例如记录日志或者继续循环\n",
    "            continue  # 发生异常时跳过当前书籍，处理下一本书籍\n",
    "\n",
    "\n",
    "    # 根据选中的一、二、三级键名生成结果文件名\n",
    "    json_file_name = '_'.join(target_keys) + '_results.json'\n",
    "\n",
    "    # 将键列表保存到JSON文件\n",
    "    js.write_json(content=result,file_path=json_file_name)\n",
    "\n",
    "# 辅助函数：收集字典键的路径\n",
    "def collect_key_paths(current_dict, current_path=[]):\n",
    "    \"\"\"递归收集字典键的路径。\"\"\"\n",
    "    if isinstance(current_dict, dict):  # 确保当前对象是字典\n",
    "        for key, value in current_dict.items():\n",
    "            new_path = current_path + [key]\n",
    "            if isinstance(value, dict):  # 如果值也是字典，则继续递归\n",
    "                yield from collect_key_paths(value, new_path)\n",
    "            else:\n",
    "                yield new_path\n",
    "    else:\n",
    "        yield current_path\n",
    "\n",
    "# 主解析函数\n",
    "def process_directory(original_dict, top_level_key, service):\n",
    "    # 检查顶层键是否存在于原始字典中\n",
    "    if top_level_key not in original_dict:\n",
    "        print(f\"指定的顶级目录'{top_level_key}'在字典中不存在。\")\n",
    "        return\n",
    "\n",
    "    # 获取指定一级目录下的所有子目录路径\n",
    "    key_paths = list(collect_key_paths({top_level_key: original_dict[top_level_key]}))\n",
    "\n",
    "    # 对每个键路径调用extract_and_parse_json\n",
    "    for path in key_paths:\n",
    "        try:\n",
    "            extract_and_parse_json(original_dict, path, service)\n",
    "            print(f\"处理完成路径: {' -> '.join(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"处理路径{' -> '.join(path)}时出错: {e}\")\n",
    "\n",
    "original_dict = js.read_json(catalog_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **主处理功能块1：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orginal_dict要把catalog_file_path设为原始资料所在的目录\n",
    "#第二个参数是指定的第一大类\n",
    "#这个环节的service最好使用glm-3-turbo，高速廉价\n",
    "process_directory(original_dict, '计算机类', service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识点第二层聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "#主函数\n",
    "def transform_function(input_list):\n",
    "    input_dict=str({'对外汉语教学':input_list})\n",
    "    pre_prompt=f'这是对外汉语教学学科一些教材的知识点：{input_dict}我希望你帮我融合以上知识点列表，对于属于对外汉语教学的知识点，相同的部分合并精炼，不同的部分互相补充，并自行聚类总结知识点，对于无关内容直接摒弃，输出的结果最好是如下结构：'\n",
    "    pro_prompt=f'恰当地设置聚类标准，使得一个类别下的内容相似度尽可能高，同时一个类别下的内容数量尽可能少。并且，这些类别确实能够组织起这个学科的知识体系'\n",
    "    prompt = js.generate_prompt(level=3,pre_prompt=pre_prompt,pro_prompt=pro_prompt, words='知识点')\n",
    "    NotSuccess = True\n",
    "    while NotSuccess:\n",
    "        response = service.ask_once(prompt)\n",
    "        result = js.parse_dict(response)\n",
    "        if result:\n",
    "            NotSuccess = False\n",
    "            print(result)\n",
    "            return result\n",
    "        else:\n",
    "            print('无结果')\n",
    "            continue\n",
    "#辅助处理函数\n",
    "def process_dict(input_dict, window_length=1800):\n",
    "    current_length = 0\n",
    "    current_sub_list = []\n",
    "    transformed_dicts = []\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        try:\n",
    "            # 将键值对转换为字符串\n",
    "            str_value = str({key:value})\n",
    "            # 统计字符长度\n",
    "            str_length = len(str_value)\n",
    "\n",
    "            # 如果累计字符长度不超过窗口，则继续累加并加入当前子字典\n",
    "            if current_length + str_length <= window_length:\n",
    "                random.shuffle(value)\n",
    "                current_sub_list.append(value)\n",
    "                current_length += str_length\n",
    "            # 否则，将当前子字典进行转换并重置\n",
    "            else:\n",
    "                transformed_dicts.append(transform_function(current_sub_list))\n",
    "                current_sub_list = []  # 重置为空字典\n",
    "                current_length = 0     # 重置长度为0\n",
    "        except Exception as e:\n",
    "            print(\"解析失败:\", e)\n",
    "\n",
    "    # 处理最后一部分\n",
    "    if current_sub_list:\n",
    "        try:\n",
    "            transformed_dicts.append(transform_function(current_sub_list))\n",
    "        except Exception as e:\n",
    "            print(\"解析失败:\", e)\n",
    "\n",
    "    return transformed_dicts\n",
    "#衔接函数\n",
    "def process_all_json_files(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    处理指定目录下所有由process_directory函数生成的JSON文件。\n",
    "\n",
    "    :param input_directory: 包含由process_directory函数生成的JSON文件的目录路径。\n",
    "    :param output_directory: 保存transform_function处理结果的目录路径。\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('_results.json'):\n",
    "            full_file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"正在处理文件: {full_file_path}\")\n",
    "            \n",
    "            # 读取JSON文件\n",
    "            with open(full_file_path, 'r', encoding='utf-8') as file:\n",
    "                input_dict = json.load(file)\n",
    "            \n",
    "            # 对文件中的每个键值对执行transform_function\n",
    "            transformed_results = process_dict(input_dict)\n",
    "            \n",
    "            # 构建输出文件路径并保存处理结果\n",
    "            output_file_path = os.path.join(output_directory, file_name.replace('_results.json', '_transformed_results.json'))\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(transformed_results, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"处理结果已保存到: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **主处理功能块2：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = r'.'\n",
    "output_directory = r'.\\\\json_output'\n",
    "process_all_json_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 近义词聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **关键设置_路径：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填你自己的Embeddings模型路径\n",
    "model_path = \"D:\\Joining\\Models\\Text2Vec_base_zh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 加载本地模型和分词器\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 确保模型处于评估模式\n",
    "model.eval()\n",
    "print(\"模型加载成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **关键设置_功能点：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_json_aggregate(input_directory, output_directory):\n",
    "    \"\"\"\n",
    "    处理指定目录下所有由process_directory函数生成的JSON文件。\n",
    "\n",
    "    :param input_directory: 包含由process_directory函数生成的JSON文件的目录路径。\n",
    "    :param output_directory: 保存transform_function处理结果的目录路径。\n",
    "    \"\"\"\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('_results.json'):\n",
    "            full_file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"正在处理文件: {full_file_path}\")\n",
    "            \n",
    "            # 读取JSON文件\n",
    "            with open(full_file_path, 'r', encoding='utf-8') as file:\n",
    "                input_dict = json.load(file)\n",
    "            \n",
    "            # 对文件中的每个键值对执行transform_function\n",
    "            transformed_results = process_dict(input_dict)\n",
    "            \n",
    "            # 构建输出文件路径并保存处理结果\n",
    "            output_file_path = os.path.join(output_directory, file_name.replace('_results.json', '_transformed_results.json'))\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(transformed_results, file, ensure_ascii=False, indent=4)\n",
    "            print(f\"处理结果已保存到: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 假设tokenizer和model已经加载\n",
    "# vocab_list是你的词汇列表\n",
    "# 新JSON文件路径\n",
    "new_json_file_path = 'Test_full_Embeddings.json'\n",
    "\n",
    "# 初始化一个字典来存储每个词和其嵌入向量\n",
    "vocab_embeddings = {}\n",
    "\n",
    "# 初始化变量以计算每秒速率\n",
    "batch_size = 100  # 定义每批次处理的单词数量\n",
    "start_position=0\n",
    "# 分批处理词汇列表\n",
    "for i in range(0, len(vocab_list), batch_size):\n",
    "    if i < start_position:\n",
    "        continue\n",
    "    else:\n",
    "        batch_words = vocab_list[i:i+batch_size]\n",
    "\n",
    "        # 如果剩余的不足一个batch的内容，作为一个batch处理\n",
    "        if len(batch_words) < batch_size:\n",
    "            inputs = tokenizer(batch_words, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        else:\n",
    "            inputs = tokenizer(batch_words[:batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "        # 更新vocab_embeddings字典\n",
    "        for j, word in enumerate(batch_words):\n",
    "            vocab_embeddings[word] = embeddings[j].tolist()\n",
    "\n",
    "        # 每处理完一个batch，存储到 JSON 文件中\n",
    "        with open(new_json_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(vocab_embeddings, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"已处理 {i + len(batch_words)} 个单词。\")\n",
    "\n",
    "print(f\"全部单词处理完毕，结果已存储到 JSON 文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = vocab_embeddings\n",
    "\n",
    "# 计算词向量之间的余弦相似度\n",
    "def cosine_similarity_score(word1, word2):\n",
    "    vector1 = mapping_dict[word1]\n",
    "    vector2 = mapping_dict[word2]\n",
    "    return cosine_similarity([vector1], [vector2])[0][0]\n",
    "\n",
    "# 判断两个词是否近义词\n",
    "def are_synonyms(word1, word2, threshold=0.8):\n",
    "    similarity_score = cosine_similarity_score(word1, word2)\n",
    "    return similarity_score >= threshold\n",
    "\n",
    "# 输出结果\n",
    "for current_set in all_sets:\n",
    "    synonyms = {}\n",
    "    for word in current_set:\n",
    "        synonyms[word] = [other_word for other_word in sum(all_sets, []) if are_synonyms(word, other_word)]\n",
    "    print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高层抽象并输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'aggregated_result.json', 'r', encoding='utf-8') as file:\n",
    "    original_dict = json.load(file)\n",
    "\n",
    "# 上级关系的字典\n",
    "upper_relations = {\n",
    "  \"语言学\": {\n",
    "    \"对外汉语教学\": [\n",
    "      \"对外汉语教学理论\",\n",
    "      \"对外汉语翻译\",\n",
    "      \"对外汉语应用\",\n",
    "      \"对外汉语背景\",\n",
    "      \"对外汉语思想史\",\n",
    "      \"对外汉语社会文化\",\n",
    "      \"对外汉语消费文化\",\n",
    "      \"对外汉语教学总览\"\n",
    "    ],\n",
    "    \"现代汉语\": [\n",
    "      \"语言点\",\n",
    "      \"教学用语\",\n",
    "      \"教师语言\",\n",
    "      \"日常用语\",\n",
    "      \"商务用语\"\n",
    "    ],\n",
    "    \"汉字教学\": [\n",
    "      \"汉字课知识点\"\n",
    "    ],\n",
    "    \"语音学\": [\n",
    "      \"语音课知识点\"\n",
    "    ],\n",
    "    \"口语教学\": [\n",
    "      \"口语课知识点\"\n",
    "    ],\n",
    "    \"听力教学\": [\n",
    "      \"听力课知识点\"\n",
    "    ],\n",
    "    \"阅读教学\": [\n",
    "      \"阅读课知识点\"\n",
    "    ],\n",
    "    \"写作教学\": [\n",
    "      \"写作课知识点\"\n",
    "    ],\n",
    "    \"文化教学\": [\n",
    "      \"文化课知识点\"\n",
    "    ],\n",
    "    \"其他教学\": [\n",
    "      \"综合课知识点\",\n",
    "      \"手工艺课知识点\",\n",
    "      \"语法练习\"\n",
    "    ]\n",
    "  },\n",
    "  \"文学\": {\n",
    "    \"文学作品\": [\n",
    "      \"文学作品\",\n",
    "      \"作家作品\"\n",
    "    ],\n",
    "    \"文学理论\": [\n",
    "      \"文艺理论\",\n",
    "      \"比较文学理论与方法\",\n",
    "      \"文学评论\",\n",
    "      \"文学与其他学科的关系\",\n",
    "      \"文学创作方法\",\n",
    "      \"文学体裁\",\n",
    "      \"经典文学导读\",\n",
    "      \"现代文学作家\",\n",
    "      \"比较文学分支学科\",\n",
    "      \"比较文学的历史与发展\",\n",
    "      \"翻译文学研究\",\n",
    "      \"跨文明比较文学\",\n",
    "      \"其他专题研究\"\n",
    "    ]\n",
    "  },\n",
    "  \"文化\": {\n",
    "    \"文化概念\": [\n",
    "      \"文化\",\n",
    "      \"中国传统文化\",\n",
    "      \"中国文化\"\n",
    "    ],\n",
    "    \"日常生活与交流\": [\n",
    "      \"日常生活话题\",\n",
    "      \"生活交际\",\n",
    "      \"日常交流\",\n",
    "      \"日常生活\"\n",
    "    ],\n",
    "    \"文化与文学交叉\": [\n",
    "      \"汉字\",\n",
    "      \"中国现当代文学\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "def aggregate_dict(upper_relations, original_dict, aggregated_dict=None, current_path=None):\n",
    "    if aggregated_dict is None:\n",
    "        aggregated_dict = {}\n",
    "    if current_path is None:\n",
    "        current_path = []\n",
    "\n",
    "    for key, value in upper_relations.items():\n",
    "        # 如果值是列表，那么我们找到了一个终端节点\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                if item in original_dict:\n",
    "                    # 构建聚合字典的结构\n",
    "                    d = aggregated_dict\n",
    "                    for path_key in current_path + [key]:\n",
    "                        d = d.setdefault(path_key, {})\n",
    "                    d[item] = original_dict[item]\n",
    "        else:\n",
    "            # 如果值是字典，递归搜索\n",
    "            aggregate_dict(value, original_dict, aggregated_dict, current_path + [key])\n",
    "\n",
    "    return aggregated_dict\n",
    "\n",
    "# 使用改进的函数进行聚合\n",
    "aggregated_dict = aggregate_dict(upper_relations, original_dict)\n",
    "print(aggregated_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
